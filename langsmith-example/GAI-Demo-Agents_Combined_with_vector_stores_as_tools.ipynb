{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1x_OZFitQBVAM_NTNc0xe_teyqYTdj6BE","timestamp":1692869139943},{"file_id":"1AA8czIERj4XsUk2Aliaa6iqd5Gf3MIoA","timestamp":1692632300445}],"authorship_tag":"ABX9TyPq8PPD4BMIu5ufZK3mH5Wl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Combine agents and vector stores\n","This notebook covers how to combine agents and vectorstores. The use case for this is that you've ingested your data into a vectorstore and want to interact with it in an agentic manner.\n","\n","The recommended method for doing so is to create a RetrievalQA and then use that as a tool in the overall agent. Let's take a look at doing this below. You can do this with multiple different vectordbs, and use the agent as a way to route between them. There are two different ways of doing this - you can either let the agent use the vectorstores as normal tools, or you can set return_direct=True to really just use the agent as a router.\n","\n","#Integrating langsmith\n","\n","Langsmith is used for demonstrating debuging. Connect to https://smith.langchain.com\n","\n","Harrison Chase, creator of LangChain was kind enough to provide us with a beta access code - **langsmith_partner_2023**\n","\n","Please feel free to provide writups, and feedback to Harrison and the @langchainAI about LangSmith. It is a work in progress, with changes implemented all the time"],"metadata":{"id":"DF1Rd_r4iXmj"}},{"cell_type":"code","execution_count":89,"metadata":{"id":"0jK7C50xiWTH","executionInfo":{"status":"ok","timestamp":1692889621574,"user_tz":240,"elapsed":9842,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"outputs":[],"source":["!pip install -q langchain openai colab_env chromadb tiktoken"]},{"cell_type":"code","source":["import colab_env\n","import langchain\n","import openai\n","import os"],"metadata":{"id":"66jDgf-rXmvk","executionInfo":{"status":"ok","timestamp":1692889621575,"user_tz":240,"elapsed":9,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":["#export the project name to langsmith\n","\n","**note you need to add this name as a project to https://smith.langchain.com**"],"metadata":{"id":"KEeeKOfQ0dOO"}},{"cell_type":"code","source":["# Connect this notebook to langsmith\n","\n","import os\n","\n","# connect notebook to langsmith\n","os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n","os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n","\n","# This key is sourced from vars.env\n","# os.environ['LANGCHAIN_API_KEY'] = '<your-api-key>'  # Uncomment and replace <your-api-key> with your actual API key\n","\n","os.environ['LANGCHAIN_PROJECT'] = 'GAI-Demo-Agents_Combined_with_vector_stores_as_tools'\n","\n","# To verify, you can print the variables\n","print(os.environ.get('LANGCHAIN_TRACING_V2'))\n","print(os.environ.get('LANGCHAIN_ENDPOINT'))\n","#print(os.environ.get('LANGCHAIN_API_KEY'))  # Uncomment if you want to print your API key (be careful with sharing your notebook)\n","print(os.environ.get('LANGCHAIN_PROJECT'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAP4yXbY5Wz1","executionInfo":{"status":"ok","timestamp":1692889621575,"user_tz":240,"elapsed":8,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"5ec2905a-a563-4bc4-b7ea-755751779190"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["true\n","https://api.smith.langchain.com\n","GAI-Demo-Agents_Combined_with_vector_stores_as_tools\n"]}]},{"cell_type":"code","source":["#pull your openai key from your vars.env or dotenv\n","openai_api_key=os.environ['OPENAI_API_KEY']"],"metadata":{"id":"8WsfvyC1YC96","executionInfo":{"status":"ok","timestamp":1692889621575,"user_tz":240,"elapsed":5,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.llms import OpenAI\n","from langchain.chains import RetrievalQA\n","from langchain.document_loaders import DirectoryLoader\n","\n","llm = OpenAI(temperature=0)"],"metadata":{"id":"Svc9u5VVifgX","executionInfo":{"status":"ok","timestamp":1692889621575,"user_tz":240,"elapsed":5,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["#from pathlib import Path\n","\n","#relevant_parts = []\n","#for p in Path(\".\").absolute().parts:\n","#    relevant_parts.append(p)\n","#    if relevant_parts[-3:] == [\"langchain\", \"docs\", \"modules\"]:\n","#        break\n","#doc_path = str(Path(*relevant_parts) / \"/content/gdrive/MyDrive/Projects/GAIHackathon0823/transcripts/day1/Talk1_-_OpenAI_Summit_-_Techstrong.txt\")"],"metadata":{"id":"KZ1UtuQUi7d5","executionInfo":{"status":"ok","timestamp":1692889621575,"user_tz":240,"elapsed":5,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["documents = []\n","# Note, I used my relative path in my google drive. If executing using terminal or vscode uncomment the line below\n","# loader = DirectoryLoader('../transcripts/', glob='**/*.txt')\n","\n","# Google Colab / Gdrive code\n","loader = DirectoryLoader('/content/gdrive/MyDrive/Projects/GAIHackathon0823/transcripts/', glob='**/*.txt')"],"metadata":{"id":"2sfoARwxCqSp","executionInfo":{"status":"ok","timestamp":1692889621575,"user_tz":240,"elapsed":5,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader\n","\n","loader = TextLoader(doc_path)\n","documents = loader.load()\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","texts = text_splitter.split_documents(documents)\n","\n","embeddings = OpenAIEmbeddings()\n","docsearch = Chroma.from_documents(texts, embeddings, collection_name=\"GAIhackathon-transcripts\")"],"metadata":{"id":"gu9K3lLyjF7a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692889623011,"user_tz":240,"elapsed":1441,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"08a2dc67-8a9f-49e8-85c9-22574aac6f43"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain.text_splitter:Created a chunk of size 1042, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1099, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1336, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1249, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1229, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1741, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1334, which is longer than the specified 1000\n"]}]},{"cell_type":"code","source":["gai_hackathon = RetrievalQA.from_chain_type(\n","    llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever()\n",")"],"metadata":{"id":"htW9vCYVjW-X","executionInfo":{"status":"ok","timestamp":1692889623011,"user_tz":240,"elapsed":3,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import WebBaseLoader"],"metadata":{"id":"AhXzaUHbkBiO","executionInfo":{"status":"ok","timestamp":1692889623011,"user_tz":240,"elapsed":2,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["#loader = WebBaseLoader(\"https://python.langchain.com/docs/\")\n","loader = WebBaseLoader(\"https://beta.ruff.rs/docs/faq/\")"],"metadata":{"id":"tpPZ35M1kCZL","executionInfo":{"status":"ok","timestamp":1692889623011,"user_tz":240,"elapsed":2,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["docs = loader.load()\n","ruff_qa_texts = text_splitter.split_documents(docs)\n","ruff_qa_db = Chroma.from_documents(ruff_qa_texts, embeddings, collection_name=\"ruff_qa\")\n","ruff_qa = RetrievalQA.from_chain_type(\n","    llm=llm, chain_type=\"stuff\", retriever=ruff_qa_db.as_retriever()\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TG02lpBDkEk2","executionInfo":{"status":"ok","timestamp":1692889624234,"user_tz":240,"elapsed":1225,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"cad369c5-e5e7-4848-e273-a7fdfd50bca7"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain.text_splitter:Created a chunk of size 1354, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1923, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1267, which is longer than the specified 1000\n"]}]},{"cell_type":"code","source":["len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT5uCOJ45naD","executionInfo":{"status":"ok","timestamp":1692889624234,"user_tz":240,"elapsed":9,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"b7987215-c80e-425e-bccc-d58b41502b37"},"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":101}]},{"cell_type":"markdown","source":["#create the agent"],"metadata":{"id":"o_q2ul2dkJmE"}},{"cell_type":"code","source":["# Import things that are needed generically\n","from langchain.agents import initialize_agent, Tool\n","from langchain.agents import AgentType\n","from langchain.tools import BaseTool\n","from langchain.llms import OpenAI\n","from langchain import LLMMathChain, SerpAPIWrapper"],"metadata":{"id":"k1SgAsYakGXd","executionInfo":{"status":"ok","timestamp":1692889624234,"user_tz":240,"elapsed":3,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["tools = [\n","    Tool(\n","        name=\"GAI Hackathon QA System\",\n","        #func=state_of_union.run,\n","        func=gai_hackathon.run,\n","        description=\"useful for when you need to answer questions about the most recent GAI Hackathon discussions. Input should be a fully formed question.\",\n","    ),\n","    Tool(\n","        name=\"Ruff QA System\",\n","        #func=ruff_qa.run,\n","        func=ruff_qa.run,\n","        description=\"useful for when you need to answer questions about Ruff (a Python Linter). Input should be a fully formed question.\",\n","    ),\n","]"],"metadata":{"id":"oUotRqpSkN-4","executionInfo":{"status":"ok","timestamp":1692889624235,"user_tz":240,"elapsed":4,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["# Construct the agent. We will use the default agent type here.\n","# See documentation for a full list of options.\n","agent = initialize_agent(\n","    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",")"],"metadata":{"id":"t-pG9b_TkSI-","executionInfo":{"status":"ok","timestamp":1692889624235,"user_tz":240,"elapsed":3,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":104,"outputs":[]},{"cell_type":"code","source":["agent.run(\n","    \"What was said about LLMs, if there are multiple statements compare and contrast all?\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"HnQD0LtTkSqP","executionInfo":{"status":"ok","timestamp":1692889632204,"user_tz":240,"elapsed":7972,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"a4946975-e87f-4730-9bb3-f80472de4cd3"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to find out what was said about LLMs.\n","Action: GAI Hackathon QA System\n","Action Input: What was said about LLMs?\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m Speaker 10 said that LLMs can be used to interpret unique terms differently and that they can be used to patch externally trained LM ER models to a specific use case. They also mentioned that using different acronyms can open up new security vectors and prompt injection.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I need to compare and contrast the statements.\n","Action: GAI Hackathon QA System\n","Action Input: What are the differences between the statements about LLMs?\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m The speaker is discussing how different LLMs interpret unique terms differently and how this can open up new security vectors and prompt injection when using a different acronym.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n","Final Answer: LLMs can be used to interpret unique terms differently and can be used to patch externally trained LM ER models to a specific use case. Using different acronyms can open up new security vectors and prompt injection. The differences between the statements are that LLMs interpret unique terms differently and using different acronyms can open up new security vectors and prompt injection.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'LLMs can be used to interpret unique terms differently and can be used to patch externally trained LM ER models to a specific use case. Using different acronyms can open up new security vectors and prompt injection. The differences between the statements are that LLMs interpret unique terms differently and using different acronyms can open up new security vectors and prompt injection.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":105}]},{"cell_type":"code","source":["agent.run(\"How would I lint a jupyter notebook?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"id":"coMRkjTWkaUQ","executionInfo":{"status":"ok","timestamp":1692889636259,"user_tz":240,"elapsed":4069,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"b36011f0-fd91-4670-a902-bd471678cf9a"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /runs\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to use a linter to check the code in the notebook\n","Action: Ruff QA System\n","Action Input: How do I lint a jupyter notebook?\u001b[0m\n","Observation: \u001b[33;1m\u001b[1;3m You can lint a Jupyter notebook by running Ruff over it with nbQA. After installing Ruff and nbQA, you can run Ruff over a notebook like so: nbqa ruff Untitled.ipynb.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n","Final Answer: To lint a Jupyter notebook, you can use Ruff with nbQA by running nbqa ruff Untitled.ipynb.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'To lint a Jupyter notebook, you can use Ruff with nbQA by running nbqa ruff Untitled.ipynb.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":106}]},{"cell_type":"markdown","source":["#Use the Agent solely as a router\n","You can also set return_direct=True if you intend to use the agent as a router and just want to directly return the result of the RetrievalQAChain.\n","\n","Notice that in the above examples the agent did some extra work after querying the RetrievalQAChain. You can avoid that and just return the result directly."],"metadata":{"id":"dcVamUe7kvxJ"}},{"cell_type":"code","source":["tools = [\n","    Tool(\n","        name=\"GAI Hackathon QA System\",\n","        func=gai_hackathon.run,\n","        description=\"useful for when you need to answer questions about the most recent GAI Hackathon dicussion. Input should be a fully formed question.\",\n","        return_direct=True,\n","    ),\n","    Tool(\n","        name=\"ruff_qa QA System\",\n","        func=ruff_qa.run,\n","        description=\"useful for when you need to answer questions about ruff_qa. Input should be a fully formed question.\",\n","        return_direct=True,\n","    ),\n","]"],"metadata":{"id":"I6UoVC3Wkbnv","executionInfo":{"status":"ok","timestamp":1692889636259,"user_tz":240,"elapsed":19,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["agent = initialize_agent(\n","    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",")"],"metadata":{"id":"nBC_iX3fk2e9","executionInfo":{"status":"ok","timestamp":1692889636259,"user_tz":240,"elapsed":18,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","source":["agent.run(\n","    \"What was said about LLMs?\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"id":"eePfG3jzk4Dv","executionInfo":{"status":"ok","timestamp":1692889638862,"user_tz":240,"elapsed":2621,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"68daaec3-958f-4267-feb2-0e22e8097609"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to find out what was said about LLMs\n","Action: GAI Hackathon QA System\n","Action Input: What was said about LLMs?\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m Speaker 10 said that LLMs have a codex of three letter acronyms and that it is interesting to think about how to take an LLM model trained externally and patch it internally for a specific use case, and what security vectors that could open up.\u001b[0m\n","\u001b[32;1m\u001b[1;3m\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["' Speaker 10 said that LLMs have a codex of three letter acronyms and that it is interesting to think about how to take an LLM model trained externally and patch it internally for a specific use case, and what security vectors that could open up.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":109}]},{"cell_type":"markdown","source":["#Multi-Hop vectorstore reasoning\n","Because vectorstores are easily usable as tools in agents, it is easy to use answer multi-hop questions that depend on vectorstores using the existing agent framework"],"metadata":{"id":"LQV2CkFflFG2"}},{"cell_type":"code","source":["tools = [\n","    Tool(\n","        name=\"GAI Hackathon QA System\",\n","        func=gai_hackathon.run,\n","        description=\"useful for when you need to answer questions about the most recent GAI Hackathon dicussion. Input should be a fully formed question, not referencing any obscure pronouns from the conversation before.\",\n","    ),\n","    Tool(\n","        name=\"ruff_qa QA System\",\n","        func=ruff_qa.run,\n","        description=\"useful for when you need to answer questions about ruff (a python linter)). Input should be a fully formed question, not referencing any obscure pronouns from the conversation before.\",\n","    ),\n","]"],"metadata":{"id":"s15ChMHOk6Rt","executionInfo":{"status":"ok","timestamp":1692889638863,"user_tz":240,"elapsed":7,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["# Construct the agent. We will use the default agent type here.\n","# See documentation for a full list of options.\n","agent = initialize_agent(\n","    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",")"],"metadata":{"id":"B_MokHiylHhb","executionInfo":{"status":"ok","timestamp":1692889638863,"user_tz":240,"elapsed":6,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["agent.run(\n","    \"How can I lint my jupyter notebook?, how would I use LLMs in the enterprise?\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"dA4Jpx0mlIwf","executionInfo":{"status":"ok","timestamp":1692889646225,"user_tz":240,"elapsed":7367,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"bdc8f665-f203-4473-b255-e9319e104f3f"},"execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to use the right tool for the job\n","Action: ruff_qa QA System\n","Action Input: How can I lint my jupyter notebook?\u001b[0m\n","Observation: \u001b[33;1m\u001b[1;3m You can lint your Jupyter Notebook by running Ruff over it. After installing Ruff and nbQA, you can run Ruff over a notebook like so: nbqa ruff Untitled.ipynb.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I need to use the right tool for the job\n","Action: GAI Hackathon QA System\n","Action Input: How would I use LLMs in the enterprise?\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m It depends on the use case. LLMs can be used to patch externally trained models to an internal use case, but this could open up new security vectors and prompt injection risks. It is important to consider the codex of three letter acronyms and special words used in the enterprise, as well as the incident management and confluence or wiki documents.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n","Final Answer: You can lint your Jupyter Notebook by running Ruff over it. When using LLMs in the enterprise, it is important to consider the codex of three letter acronyms and special words used in the enterprise, as well as the incident management and confluence or wiki documents.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'You can lint your Jupyter Notebook by running Ruff over it. When using LLMs in the enterprise, it is important to consider the codex of three letter acronyms and special words used in the enterprise, as well as the incident management and confluence or wiki documents.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":112}]}]}