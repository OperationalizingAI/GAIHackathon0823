{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1x_OZFitQBVAM_NTNc0xe_teyqYTdj6BE","timestamp":1692869139943},{"file_id":"1AA8czIERj4XsUk2Aliaa6iqd5Gf3MIoA","timestamp":1692632300445}],"authorship_tag":"ABX9TyMKQPxWOs+3dU3lAiEqZYP5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Combine agents and vector stores\n","This notebook covers how to combine agents and vectorstores. The use case for this is that you've ingested your data into a vectorstore and want to interact with it in an agentic manner.\n","\n","The recommended method for doing so is to create a RetrievalQA and then use that as a tool in the overall agent. Let's take a look at doing this below. You can do this with multiple different vectordbs, and use the agent as a way to route between them. There are two different ways of doing this - you can either let the agent use the vectorstores as normal tools, or you can set return_direct=True to really just use the agent as a router.\n","\n","#Integrating langsmith\n","\n","Langsmith is used for demonstrating debuging. Connect to https://smith.langchain.com\n","\n","Harrison Chase, creator of LangChain was kind enough to provide us with a beta access code - **langsmith_partner_2023**\n","\n","Please feel free to provide writups, and feedback to Harrison and the @langchainAI about LangSmith. It is a work in progress, with changes implemented all the time"],"metadata":{"id":"DF1Rd_r4iXmj"}},{"cell_type":"code","execution_count":40,"metadata":{"id":"0jK7C50xiWTH","executionInfo":{"status":"ok","timestamp":1692872432277,"user_tz":240,"elapsed":5778,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"outputs":[],"source":["!pip install -q langchain openai colab_env chromadb tiktoken"]},{"cell_type":"code","source":["import colab_env\n","import langchain\n","import openai\n","import os"],"metadata":{"id":"66jDgf-rXmvk","executionInfo":{"status":"ok","timestamp":1692872432277,"user_tz":240,"elapsed":17,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["#export the notebook name to langsmith"],"metadata":{"id":"KEeeKOfQ0dOO"}},{"cell_type":"code","source":["# Connect this notebook to langsmith\n","\n","import os\n","\n","# connect notebook to langsmith\n","os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n","os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n","\n","# This key is sourced from vars.env\n","# os.environ['LANGCHAIN_API_KEY'] = '<your-api-key>'  # Uncomment and replace <your-api-key> with your actual API key\n","\n","os.environ['LANGCHAIN_PROJECT'] = 'GAI-Demo-Agents_Combined_with_vector_stores_as_tools'\n","\n","# To verify, you can print the variables\n","print(os.environ.get('LANGCHAIN_TRACING_V2'))\n","print(os.environ.get('LANGCHAIN_ENDPOINT'))\n","#print(os.environ.get('LANGCHAIN_API_KEY'))  # Uncomment if you want to print your API key (be careful with sharing your notebook)\n","print(os.environ.get('LANGCHAIN_PROJECT'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAP4yXbY5Wz1","executionInfo":{"status":"ok","timestamp":1692872432277,"user_tz":240,"elapsed":16,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"e748fad9-87d1-4daa-d9e4-fdfa33723440"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["true\n","https://api.smith.langchain.com\n","GAI-Demo-Agents_Combined_with_vector_stores_as_tools\n"]}]},{"cell_type":"code","source":["#pull your openai key from your vars.env or dotenv\n","openai_api_key=os.environ['OPENAI_API_KEY']"],"metadata":{"id":"8WsfvyC1YC96","executionInfo":{"status":"ok","timestamp":1692872432277,"user_tz":240,"elapsed":11,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.llms import OpenAI\n","from langchain.chains import RetrievalQA\n","from langchain.document_loaders import DirectoryLoader\n","\n","llm = OpenAI(temperature=0)"],"metadata":{"id":"Svc9u5VVifgX","executionInfo":{"status":"ok","timestamp":1692872432278,"user_tz":240,"elapsed":12,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["#from pathlib import Path\n","\n","#relevant_parts = []\n","#for p in Path(\".\").absolute().parts:\n","#    relevant_parts.append(p)\n","#    if relevant_parts[-3:] == [\"langchain\", \"docs\", \"modules\"]:\n","#        break\n","#doc_path = str(Path(*relevant_parts) / \"/content/gdrive/MyDrive/Projects/GAIHackathon0823/transcripts/day1/Talk1_-_OpenAI_Summit_-_Techstrong.txt\")"],"metadata":{"id":"KZ1UtuQUi7d5","executionInfo":{"status":"ok","timestamp":1692872432278,"user_tz":240,"elapsed":12,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["documents = []\n","# Note, I used my relative path in my google drive. If executing using terminal or vscode uncomment the line below\n","# loader = DirectoryLoader('./transcripts/', glob='**/*.txt')\n","\n","# Google Colab / Gdrive code\n","loader = DirectoryLoader('/content/gdrive/MyDrive/Projects/GAIHackathon0823/transcripts/', glob='**/*.txt')"],"metadata":{"id":"2sfoARwxCqSp","executionInfo":{"status":"ok","timestamp":1692872432278,"user_tz":240,"elapsed":11,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader\n","\n","loader = TextLoader(doc_path)\n","documents = loader.load()\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","texts = text_splitter.split_documents(documents)\n","\n","embeddings = OpenAIEmbeddings()\n","docsearch = Chroma.from_documents(texts, embeddings, collection_name=\"GAIhackathon-transcripts\")"],"metadata":{"id":"gu9K3lLyjF7a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692872432989,"user_tz":240,"elapsed":722,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"e5b005b4-68b7-4ed8-bb5b-1d0d353b67b1"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain.text_splitter:Created a chunk of size 1042, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1099, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1336, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1249, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1229, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1741, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1334, which is longer than the specified 1000\n"]}]},{"cell_type":"code","source":["gai_hackathon = RetrievalQA.from_chain_type(\n","    llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever()\n",")"],"metadata":{"id":"htW9vCYVjW-X","executionInfo":{"status":"ok","timestamp":1692872432989,"user_tz":240,"elapsed":4,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import WebBaseLoader"],"metadata":{"id":"AhXzaUHbkBiO","executionInfo":{"status":"ok","timestamp":1692872432989,"user_tz":240,"elapsed":3,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["#loader = WebBaseLoader(\"https://python.langchain.com/docs/\")\n","loader = WebBaseLoader(\"https://beta.ruff.rs/docs/faq/\")"],"metadata":{"id":"tpPZ35M1kCZL","executionInfo":{"status":"ok","timestamp":1692872433167,"user_tz":240,"elapsed":181,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["docs = loader.load()\n","ruff_qa_texts = text_splitter.split_documents(docs)\n","ruff_qa_db = Chroma.from_documents(ruff_qa_texts, embeddings, collection_name=\"ruff_qa\")\n","ruff_qa = RetrievalQA.from_chain_type(\n","    llm=llm, chain_type=\"stuff\", retriever=ruff_qa_db.as_retriever()\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TG02lpBDkEk2","executionInfo":{"status":"ok","timestamp":1692872434537,"user_tz":240,"elapsed":1372,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"88df71e7-3454-41d2-cfee-7a13e80b3639"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain.text_splitter:Created a chunk of size 1354, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1923, which is longer than the specified 1000\n","WARNING:langchain.text_splitter:Created a chunk of size 1267, which is longer than the specified 1000\n"]}]},{"cell_type":"code","source":["len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT5uCOJ45naD","executionInfo":{"status":"ok","timestamp":1692872434537,"user_tz":240,"elapsed":4,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"251c88ac-cdd4-4275-915b-60e7b9370f88"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["#create the agent"],"metadata":{"id":"o_q2ul2dkJmE"}},{"cell_type":"code","source":["# Import things that are needed generically\n","from langchain.agents import initialize_agent, Tool\n","from langchain.agents import AgentType\n","from langchain.tools import BaseTool\n","from langchain.llms import OpenAI\n","from langchain import LLMMathChain, SerpAPIWrapper"],"metadata":{"id":"k1SgAsYakGXd","executionInfo":{"status":"ok","timestamp":1692872434537,"user_tz":240,"elapsed":3,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["tools = [\n","    Tool(\n","        name=\"GAI Hackathon QA System\",\n","        #func=state_of_union.run,\n","        func=gai_hackathon.run,\n","        description=\"useful for when you need to answer questions about the most recent GAI Hackathon discussions. Input should be a fully formed question.\",\n","    ),\n","    Tool(\n","        name=\"Ruff QA System\",\n","        #func=ruff_qa.run,\n","        func=ruff_qa.run,\n","        description=\"useful for when you need to answer questions about Ruff (a Python Linter). Input should be a fully formed question.\",\n","    ),\n","]"],"metadata":{"id":"oUotRqpSkN-4","executionInfo":{"status":"ok","timestamp":1692872434538,"user_tz":240,"elapsed":4,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# Construct the agent. We will use the default agent type here.\n","# See documentation for a full list of options.\n","agent = initialize_agent(\n","    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",")"],"metadata":{"id":"t-pG9b_TkSI-","executionInfo":{"status":"ok","timestamp":1692872434538,"user_tz":240,"elapsed":4,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["agent.run(\n","    \"What was said about LLMs, if there are multiple statements compare and contrast all?\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"HnQD0LtTkSqP","executionInfo":{"status":"ok","timestamp":1692872440845,"user_tz":240,"elapsed":6311,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"9bd76f02-e04b-4f47-8032-3a4983a4328f"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to find out what was said about LLMs.\n","Action: GAI Hackathon QA System\n","Action Input: What was said about LLMs?\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m Speaker 10 said that LLMs can be used to patch externally trained models to a use case, and that this can open up new security vectors and prompt injection.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I need to compare and contrast the statements.\n","Action: GAI Hackathon QA System\n","Action Input: What other statements were made about LLMs?\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m Speaker 1 mentioned that they don't believe in the bigger problem set of continuing to vote on security. Speaker 2 mentioned looking at the OWASP Top 10 tokens.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n","Final Answer: LLMs can be used to patch externally trained models to a use case, which can open up new security vectors and prompt injection. This is in contrast to the opinion of Speaker 1, who does not believe in the bigger problem set of continuing to vote on security, and Speaker 2, who mentioned looking at the OWASP Top 10 tokens.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'LLMs can be used to patch externally trained models to a use case, which can open up new security vectors and prompt injection. This is in contrast to the opinion of Speaker 1, who does not believe in the bigger problem set of continuing to vote on security, and Speaker 2, who mentioned looking at the OWASP Top 10 tokens.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["agent.run(\"How would I lint a jupyter notebook?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"coMRkjTWkaUQ","executionInfo":{"status":"ok","timestamp":1692872448862,"user_tz":240,"elapsed":8044,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"183d365c-5df4-4f6d-d223-54c4a6085785"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /runs\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to use a linter to check the code in the notebook\n","Action: Ruff QA System\n","Action Input: How do I lint a jupyter notebook?\u001b[0m\n","Observation: \u001b[33;1m\u001b[1;3m You can lint a Jupyter Notebook by running Ruff over it with nbqa. For example, nbqa ruff Untitled.ipynb will lint Untitled.ipynb. You can also opt in to linting Jupyter Notebook (.ipynb) files by adding the *.ipynb pattern to your include setting in your pyproject.toml file.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know how to lint a jupyter notebook\n","Final Answer: You can lint a Jupyter Notebook by running Ruff over it with nbqa. For example, nbqa ruff Untitled.ipynb will lint Untitled.ipynb. You can also opt in to linting Jupyter Notebook (.ipynb) files by adding the *.ipynb pattern to your include setting in your pyproject.toml file.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'You can lint a Jupyter Notebook by running Ruff over it with nbqa. For example, nbqa ruff Untitled.ipynb will lint Untitled.ipynb. You can also opt in to linting Jupyter Notebook (.ipynb) files by adding the *.ipynb pattern to your include setting in your pyproject.toml file.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["#Use the Agent solely as a router\n","You can also set return_direct=True if you intend to use the agent as a router and just want to directly return the result of the RetrievalQAChain.\n","\n","Notice that in the above examples the agent did some extra work after querying the RetrievalQAChain. You can avoid that and just return the result directly."],"metadata":{"id":"dcVamUe7kvxJ"}},{"cell_type":"code","source":["tools = [\n","    Tool(\n","        name=\"GAI Hackathon QA System\",\n","        func=gai_hackathon.run,\n","        description=\"useful for when you need to answer questions about the most recent GAI Hackathon dicussion. Input should be a fully formed question.\",\n","        return_direct=True,\n","    ),\n","    Tool(\n","        name=\"ruff_qa QA System\",\n","        func=ruff_qa.run,\n","        description=\"useful for when you need to answer questions about ruff_qa. Input should be a fully formed question.\",\n","        return_direct=True,\n","    ),\n","]"],"metadata":{"id":"I6UoVC3Wkbnv","executionInfo":{"status":"ok","timestamp":1692872448863,"user_tz":240,"elapsed":20,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["agent = initialize_agent(\n","    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",")"],"metadata":{"id":"nBC_iX3fk2e9","executionInfo":{"status":"ok","timestamp":1692872448863,"user_tz":240,"elapsed":19,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["agent.run(\n","    \"What was said about LLMs?\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"id":"eePfG3jzk4Dv","executionInfo":{"status":"ok","timestamp":1692872450738,"user_tz":240,"elapsed":1894,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"8096bf60-7511-4b61-e59b-6a6452468fd3"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to find out what was said about LLMs\n","Action: GAI Hackathon QA System\n","Action Input: What was said about LLMs?\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m Speaker 10 said that LLMs can be used to patch externally trained models to a use case, and that this can open up new security vectors and prompt injection.\u001b[0m\n","\u001b[32;1m\u001b[1;3m\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["' Speaker 10 said that LLMs can be used to patch externally trained models to a use case, and that this can open up new security vectors and prompt injection.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["#Multi-Hop vectorstore reasoning\n","Because vectorstores are easily usable as tools in agents, it is easy to use answer multi-hop questions that depend on vectorstores using the existing agent framework"],"metadata":{"id":"LQV2CkFflFG2"}},{"cell_type":"code","source":["tools = [\n","    Tool(\n","        name=\"GAI Hackathon QA System\",\n","        func=gai_hackathon.run,\n","        description=\"useful for when you need to answer questions about the most recent GAI Hackathon dicussion. Input should be a fully formed question, not referencing any obscure pronouns from the conversation before.\",\n","    ),\n","    Tool(\n","        name=\"ruff_qa QA System\",\n","        func=ruff_qa.run,\n","        description=\"useful for when you need to answer questions about ruff (a python linter)). Input should be a fully formed question, not referencing any obscure pronouns from the conversation before.\",\n","    ),\n","]"],"metadata":{"id":"s15ChMHOk6Rt","executionInfo":{"status":"ok","timestamp":1692872450738,"user_tz":240,"elapsed":5,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# Construct the agent. We will use the default agent type here.\n","# See documentation for a full list of options.\n","agent = initialize_agent(\n","    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",")"],"metadata":{"id":"B_MokHiylHhb","executionInfo":{"status":"ok","timestamp":1692872450739,"user_tz":240,"elapsed":5,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["agent.run(\n","    \"How can I lint my jupyter notebook?, how would I use LLMs in the enterprise?\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"id":"dA4Jpx0mlIwf","executionInfo":{"status":"ok","timestamp":1692872461342,"user_tz":240,"elapsed":10607,"user":{"displayName":"Colin McNamara","userId":"11900919394687908359"}},"outputId":"ca54e0ce-800d-49f9-a746-ea62da05e4c8"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to use the right tool for the job\n","Action: ruff_qa QA System\n","Action Input: How can I lint my jupyter notebook?\u001b[0m\n","Observation: \u001b[33;1m\u001b[1;3m You can lint your Jupyter Notebook by running Ruff over it. After installing Ruff and nbQA, you can run Ruff over a notebook like so: nbqa ruff Untitled.ipynb. To opt in to linting Jupyter Notebook (.ipynb) files, add the *.ipynb pattern to your include setting in your pyproject.toml.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I need to use the right tool for the job\n","Action: GAI Hackathon QA System\n","Action Input: How would I use LLMs in the enterprise?\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m It depends on the use case. LLMs can be used to patch externally trained models to an internal use case, which can open up new security vectors and prompt injection opportunities. It is important to understand the codex of three letter acronyms and special words used in the enterprise to ensure the LLM is properly configured.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n","Final Answer: You can lint your Jupyter Notebook by running Ruff over it. To opt in to linting Jupyter Notebook (.ipynb) files, add the *.ipynb pattern to your include setting in your pyproject.toml. LLMs can be used to patch externally trained models to an internal use case, which can open up new security vectors and prompt injection opportunities. It is important to understand the codex of three letter acronyms and special words used in the enterprise to ensure the LLM is properly configured.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'You can lint your Jupyter Notebook by running Ruff over it. To opt in to linting Jupyter Notebook (.ipynb) files, add the *.ipynb pattern to your include setting in your pyproject.toml. LLMs can be used to patch externally trained models to an internal use case, which can open up new security vectors and prompt injection opportunities. It is important to understand the codex of three letter acronyms and special words used in the enterprise to ensure the LLM is properly configured.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":63}]}]}