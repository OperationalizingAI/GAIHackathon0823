Unknown Speaker  0:01  
with HDMI

John Willis  0:04  
because we had HDMI working with Did you do anything just to get us on TV to get the HDMI working? What do we do?

Unknown Speaker  0:28  
Probably don't have an agent

Unknown Speaker  0:40  
me the importance

Unknown Speaker  0:53  
yeah nice

Unknown Speaker  0:59  
okay should you try turning it on

Speaker 2  1:09  
desk always ask me is your computer like me I hate calling

John Willis  1:19  
Alright All right so we're gonna

Unknown Speaker  1:30  
do no harm. Awesome. We put that.

Speaker 3  1:33  
So we were looking at safety and security and governance. And so we started by just thinking about you know what, what are the goals in terms of safety, security resilience for these AI models? And so from a security perspective, usually what we're talking about is protecting sensitive data. You know, there's a there's known attacks like data exfiltration attacks and model inversion attacks that you want to protect against. First, so that that's sort of the security domain for safety. We sort of summarize that is do no harm. And the idea there is, this is all this sort of undesirable effects that aren't about leaking sensitive data, but still cause harm. So like things like bias in the model. If you're having an AI agent attacking that autonomous manner, and so it's making decisions and performing actions automatically without a human in the loop that there's potential for undesirable effects. You would want to try and get it down the line, you know, the scope of actions that it might take and whether any of those violate some some notion of what's desirable. And then resilience is really about predictable behavior. That was sort of the high level description of that, that we came up with. And so hallucinations falls in that category, because those nations are unpredictable, that sort of opposite definition as loose nation, right? It's like, this isn't the model. And how you expect right I give it a prompt that seemed reasonable. I came up with something out of left field. So in terms of methods for defending against these, the first one was just setting reasonable expectations. I feel like people you know, we struggle with having safe developers adopted security mindset when they're coding so that you know we can get more secure software, like we're so much farther from having that or deployment of AI, you know, like having a correct mental model for what is the threat space look like? What's possible from an attacker perspective. So things like the fact that it's data tied to a model whether it be a training, fine tuning or opting to be assumed to be shared with anyone that has access to the model, right, given some of the results around even blackbox methods for extracting training data that like you should just have that as a baseline. And if you're not comfortable with that, then you should sort of re evaluate how you're deploying AI and what you're getting into.

Unknown Speaker  3:57  
So whatever, you know, whatever you come

Speaker 3  3:59  
up with, in terms of comfort level for deploying these things, you know, governance is the method for ensuring that you're doing you're doing what you say you're gonna do, right? You're saying we're comfortable deploying this, you know, with these controls in place with this sort of architecture, with these checks in the pipeline. You know, governance is all about making sure that happens. And then Red Teaming is about detecting unanticipated vulnerabilities. You've done all these protections, you've implemented your best practices, you come up with your comfort, comfortable deployment scenario, and then the red team tries to sort of break all of that and say, Oh, well, you didn't think about this, right? You didn't think about a user interacting with it. And that's not the way and then sort of, I think that is really one of the key things that we want to spend a lot of time developing throughout the week. And as we wrap this up, is defense in depth and what that means you know, that's a core concept in software security. I think it's an important concept here as well. So what does that look like with AI models? And so you know, things like having points of control, you know, deploying controls, but also testing and monitoring, restricting access, you know, what, what sort of API are available to your model? How permissive is that a guy does it give you you know, the top five results? Does it give you the probability the top results or does it just give you the top result like those matters and attackers?

Unknown Speaker  5:18  
So wrapping this up into a pretty picture

Speaker 4  5:27  
wrapping this up and dementia sort of looking at it holistically from a bit of left where I was struggling about you know, going from sort of four

Unknown Speaker  5:35  
areas of

Speaker 4  5:38  
managing or operational model we have consumption so users read team, and it seemed somewhat defensively that each of these areas what are the need to change to restructure or restrict? How do we do this? So we started to double click down on each of these boxes. What does it mean to provide specific guidance on defensive depth holistically, but at each of these stage, we separated generative AI to development and operations. So here's where you're actually running. So you're monitoring things such as prop injection, what can you look for? Here development, things like what you're looking at as a likelihood of reconstruction of private material, and then controls and permission checks down here. So the idea is what can we do at each stage to provide what does it mean to defend these stages and as they inherit one price, and receive IT governance, the governance basically has we do this governance approach is validated when we're doing more safe. So these are what we're doing. But as we talked about, like monitoring how many times you suddenly turn on monitoring or something and nobody knew about it until the next thing big thing happened. And so it really governances two or three lines of defense, just validating and then I'll just go one before there's not much here this may look familiar to John, here, basically to this whole idea that if you do each of the steps, how do we describe each of the steps and then this is just the start consumption? What are your inputs and outputs, your consumption, your risks, your controls your actors to actions? And so as we go through and sort of talking about this, how can we fill this in and provide some very more explicit guidance? And that's about it.

Speaker 5  7:22  
What are next steps with this? This is a new game getting started with this. We want to take this next. What's the next level of digging down into it?

Speaker 4  7:51  
Jin AI.

Speaker 4  8:10  
Here's the thing with achieve thinking about bringing up this give it a bigger picture. So you may want to call us all the stuff that to me, maybe to the geeky, they have a mental model, but there's this whole other world of people that is assumption doesn't really have. And so this is that way to start and start to build that model. And how can we align this useful research

Speaker 5  8:31  
we're going to in our conversations today model seems like it's an all of the pieces and parts but as I keep coming back in my mind to identifying what their what your quality targets are being able to test against your quality targets and security to me as a as a quality aspect of polyethylene,

Unknown Speaker  8:53  
because they're also both injection that you guys should be considering. Oh, man.

Speaker 3  8:59  
Well, so yeah, I think that's sort of in the tested category. Yeah. Trying to make it fail in various ways. And terms of quality.

Speaker 3  9:24  
like no way to provide certain guarantees regarding data leakage, but integrated utility, right? And so it needs to be part of the conversation with you know, what sort of value are we trying to provide? Can we take a hit to utility to enforce certain security? It might be interesting to

Speaker 5  9:39  
also go back against the like a wasp just put out their top 10 Hello, people are buildings. A lot of them are more I posted in a lot of them are more people oriented than this technology. But it might be interesting to also figure out if those can map in are connected to this or not. The other thing that I'm wondering about for all of this is how do we take this information and kind of get ops guidance code? How do we take all of this and put

John Willis  10:12  
it into I've been thinking about that till tomorrow, but I think this is starting to evolve more like what we do gene that maybe at the end of the three days should continue. And I don't know that I didn't think of that early on, but I also was thinking about the artifacts that you know, one of the things you do with genius spent three days and then you literally then spend the next couple of months working part time to have a drop dead day where you literally produce an artifact. So I think we can have that discussion tomorrow about should we or we could have an hour but we probably it'd be nice to get ideas. I get

Unknown Speaker  10:53  
I have a tendency to really go after

John Willis  10:56  
guidances code meaning Can we join using a good mentality so I can we can make as a group, can we be the fifth folks who are proposing what the PRS are or for evaluating PRs, as people are putting forth what their changes can be? So put it out there in the public arena. But then we want to guide people forward. How do you drive a conversation? How somebody submit a PR and the conversation again? It's like the directories there and you should if you have any issues with

Unknown Speaker  11:47  
this next thing Curtis came to

Speaker 6  11:56  
Patrick anyone like it everybody's like yeah yeah appreciate

Unknown Speaker  12:05  
it

Unknown Speaker  12:21  
Okay,

John Willis  12:27  
I'm Christopher in gmail.com And

Unknown Speaker  12:42  
what did you use to do that? Oh, this one's they laid on the floor. Oh looking at true how to do that you

Speaker 7  13:00  
on the corner start getting so there's plugins right now, for gems.

Unknown Speaker  13:17  
Or when you're coding

Speaker 7  13:20  
Yeah. y'all tell me when you're ready to do it. We are. Bill Bell. cool. We have a discussion focused around pipelines. Right. So where do Where do pipelines fit into generative AI models? Where we centered our discussions around a vision around a visualization. We started with this concept of data, how do we deal with our data? And some of the interesting discussions that came out of this are in the QA process and our development process, there's a lot of parallels to ml and ml ops in data ops, where without data standardization, without proper without proper tools, and and, I guess, maturity around this, they're around how we handle our data. They're larger opportunities for leakages compliance issues making your waves there.

Speaker 7  14:35  
The third grouping is airlines that they that they access now each of these areas. We have to reference in our pipelines. Right. And one interesting thing that came out of that is we have our data, we have our code, we have our models, and then we have our somewhere our outputs, right? Whether the streaming application or Southland documents, whether whatever, right that as this entire flow, we have to capture, we have to pull the data we have to create a context to provide that back into service. We have we have to be able to provide this in a way that in this new way of making use applications we have this notions of like tools, we have agents we have we have software which we make, which may execute prompts to decide to talk to another agent to decide what's next steps and those actions aren't. Right. So a huge amount of ossification. What might be happening error code, which is the pipeline for either promoting between dev tests QA but also in production, we, without utilization of new tools and techniques lack lot of visibility into this. am I missing anything?

Speaker 8  15:52  
So we added them is that the three groups or four groups they like curl more operationally, we discussed about them all having their first name. They're all adding their test pipeline and we're all having their own defects that they created. It's not one pipeline. It's multiple versions. Delivered by all dependencies that we have in respect that you're we're even creating more dependencies, right? So we kind of have to kind of see how they're aligning our we're importing all the dependencies from there. And then the feedback we're not just doing the observable, the context. So the context, which means if you're capturing observability, you cannot capture it was a problem with that version. Though it's the problem given the context given the user given the LLM, it was us. So it becomes like a lot more complex to come to the observability at that time. But the cool thing is, if you capture all that data, we actually feed it into a new set of data. Which we call behavioral data, or complex data, which can serve as a memory what the user gets. So this is generic knowledge. This is user behavior, and then we start mixing. So that's kind of discussed in the pipeline. So the pipeline is just

Speaker 7  17:30  
it's kind of like the whole thing. Like a tracer captured. It could be a tracer could be just

Speaker 8  17:38  
depends on how but you found a company without which is very hard to

John Willis  17:46  
to like do very similar what's going on with like, so distributed tracing, like it could actually be just a higher level capture prompt capture all this sort of high level some metrics, or you could turn on like, distributed tracing? In depth. Right, that's right.

Unknown Speaker  18:04  
Because it's one of the things in the next

John Willis  18:06  
chapter in the next coming. And I know I wanted to table that like, even sort of in like the pull request idea, like, this could be stuff that we could actually sort of maybe have discussions with opentelemetry Like Like literally sort of, like succeeding like, Hey, have you guys thought about distributed tracing for Yeah, so it's just good, just good.

Speaker 7  18:29  
One interesting finding that came out of this in the koplayer, right, where we're defining our agents and you brought this up. In the context of several impromptu engineering, we define SC series A not just a sequential chain or a conversation router, but we define a group of tools and agents that are specialized and specialized in their function. The agents are communicating with each other with each other, even some of our hackathon cases through through three of Python, but the larger pacemakers connected through Kafka there's a series of agents that are coming together and that these agents are affected that have a series of templates and LM chains inside of them. And so there's emerging patterns where these prompt templates are being pulled for from an extra resource. So we have this parallel to the software supply chain security issues that we've seen recently over the years, right. But now, take that and put in for one use case of content, prompt injection, right? And as we have these layered prompts to these nested prompts, these functions that they're referencing, there's a whole new data management problem and in the context of the smartest smart people it's it's daunting enough. Now you have people like me, who are typing into perplexity with little bit of snippets of code and coming up with improved code, right. So the the surface where it used to be 10 to 20% of your company has developers having the emphasis, now you have your business, right, and you have your 80% that are playing with and improving their prompt templates or changing the characterization of the applications and are seeing that value coming back. So I know for myself, I mean, here, right? So we have a whole new a whole new area to increase our test coverage to integrate robust pipelines. And this notion of like individual pipelines to integrate together a whole new layers of complexity a whole new layers of vulnerability. We need captured our deliverables within you know, really thinking how do we take architectural principles that we have in software development and, and expand them? So how do we take these these architectural principles and map them expand them in a way that be understood by our existing reference our existing references, but then put in the context of this new app? What I suspect and maybe the lack of lack of knowledge, lack of understanding but feels like it's a it's a new development paradigm to 20 buzzword. We're now developing agents where it's like right now, you know, I'm writing chains and I've started writing agents and I started writing agents that that validate or that that reference agents, so in a sense, I started as a worker. Then I became an AI accelerated worker through using external tools, they creating external tools. And now to the point where I'm expanding, I'm kicking off a autonomous agent that's kind of living there in the codebase that can be referenced by me or others. And using that as a use case, that general business users that we're now having to really think of this pipeline needs to be expanded infrastructure needs to be expanded. The training and education needs to be expanded, you know, there's a whole whole new set of set of challenges. and I were talking about this, you know, they're this kind of the gizmo problem. Remember dismountable for everything compute after midnight, and don't give them water. Right? Well, beings were spawning corresponding awesome some grounds. Right? We have opportunity to spawn on some Gremlins, that can really recall things but can really destroy life. And the that's where we're going to do everything we're down to that. Big we didn't talk about access control. That's me. That was my contribution.

John Willis  22:31  
His foot in the picture. That's Mark

Unknown Speaker  22:36  
Collins mid journey. I guess we haven't really decided on a lot. You know, we talked a little bit about next steps within this in that

Speaker 7  22:49  
tying in basic architectural principles and creating creating a reference pipeline, where we have small examples of multiple agents, each of them with a release pipeline and integration pipeline along with like, that their reference model which can be described, visually described and ideally have a test integration that we can use use to pass to other teams, maybe a good start from this, or next steps.

John Willis  23:16  
I think another question we should sort of ask and go maybe to one Do you need any help from anybody wish to ask that in any help from other teams? I mean, I don't have to have an answer. I don't

Unknown Speaker  23:26  
know the answer better.

John Willis  23:31  
She keep asking that at the end of every debrief.

Speaker 7  23:35  
And I think that I think the best answer would be as we build the simplest implementation to capture to capture issues, and to make sure that necessarily on the flip side, we have we have the tools available to us rapidly prototype. It's gonna take us some relators to finding cool reference architectures.

Unknown Speaker  24:03  
That represents what it could be.

Speaker 7  24:04  
Yeah, I'd love to see a couple repos with some GitHub actions diagram that amounts to something that just like I've had such great experience where people put up their their early testing commendations were very nasty. That's really helped me poor my understanding around it and you can work in the space with a much higher level.

Speaker 8  24:22  
I feel like this. So my assumption is, everybody has been working together in production, right? So you're, we're getting close to like a year and this is where it gets fuzzy the testing. Guys, this is where most of the I found a lack of content is the rescuer. Okay, it is expanded so

Speaker 7  24:54  
yeah, I can I can think of like a small Python implementation with like a streamlined web interface that we would play through a pipeline and define like two or three agents with maybe two separate vector stores on the back end, or we could consolidate those back together with demonstrate and enough features and functions. might suggest a short term. Use blanks put them like chain sided there, so we could use that as something we'd show today to do the cash recall

Unknown Speaker  25:25  
and deliver context.

Unknown Speaker  25:28  
Liquid I'm very impressed by your observation about what I would say future work. Looks like in terms of life, development. Business. There's a governance or at least some kind of auditing.

Unknown Speaker  25:53  
still stand alone.

Speaker 7  25:57  
Yeah, absolutely. I can tell it's tied to the questions. We're going to see if there's some tests that are written that can be delivered by their teams. We look into that. Those would be very valuable. Uses tests. At some point we need to have some sort of mentors. I think I think it would give you all the don't kill a gap

Unknown Speaker  26:21  
test. That's right. The testing tries a red team on inverses and captures, reports on

Unknown Speaker  26:30  
time that he took tests on the idea of regression testing,

Unknown Speaker  26:33  
and how you lubricity

Speaker 4  26:38  
regression testing from a software perspective, it's I gave you an example. That's actually what I input. So how would you look at a span of acceptable results?

Speaker 7  26:53  
Well, this is where our discussion centered around the use of LM agents in our pipelines and or in our QA infrastructure to be able to score and evaluate the performance of our code as the different aspects of our code. You know, my personal thought is just the same as when we're when we're exit act when we're looking for a concept in the vector store right and we have Jason concepts next to it. Having the habit of having a similar or to the earlier discussion, going instead of a star

Speaker 4  27:25  
chain to remind you, right, if you've been our tests where we're using a refined chain tax and ask it to execute a test, and then they align itself is looking out for sure that it's testing in suggesting different types of ways in testing, red teaming, or whatever. Maybe that maybe that's something interesting to explore. To model and I'm going to build a model that I run basically a series of prompts the exact same prompts. you know, I look at the outputs and I basically asked something else, a model model that you know, Model A our model they gave me these are these prompts about this or is there gases or material difference or whether they was that question you asked because you're training yourself on a small model to basically say look at the differences and responses. And then you can define a threshold and it gets outside somebody's control.

Speaker 7  28:28  
Well, that's a good example of you know, we look at the different components, right. So the days we come in my Chase performance, the people, the models and the doctor we put over it might change if important

Unknown Speaker  29:03  
So I think so, in the early days, I was

Speaker 8  29:12  
reusing your tests for your monitoring. So this was the same thing what I see happening right now. So you're adding certain tests for your code, but then you're doing the observability of the same things happening in production because you know, they might have different examples or different data because it's the end user. But the techniques are kind of the same and the models that you're using for verification are the same. And then what you do is actually if you have examples that are not good and not fitting in, have you promoted them to your test data, and then you use that back again, so I think they're both related

Unknown Speaker  29:49  
and

Unknown Speaker  29:50  
achieving the same. Trying to

Speaker 3  30:03  
Is that something you guys planned to talk about and sort of what you're developing?

Unknown Speaker  31:38  
doesn't have to boil the ocean. And that should provide a point that we can integrate other teams works

Unknown Speaker  31:46  
is everything we do. This week has been MVP.

Speaker 5  31:49  
We're selling all the seeds for ourselves right now. But let's get an MVP out there and let's have the rest of the industry help help us with this course. Absolutely. Cool. Thank you guys.

Unknown Speaker  32:13  
whoever's next.

Unknown Speaker  32:21  
Sorry for disturbing the

Unknown Speaker  32:31  
effects Okay, so just Yeah

Unknown Speaker  32:39  
yeah I actually have a whole bunch of words.

Unknown Speaker  33:08  
Insurance ready to get one of those HDMI

Unknown Speaker  33:23  
I got one right here we have the lucky he's got one

Unknown Speaker  33:30  
right here. Awesome.

Unknown Speaker  33:31  
guys sales for co2

Speaker 6  33:51  
salesman 29

Unknown Speaker  34:20  
Here rocking alright guys so

Unknown Speaker  34:27  
right breaker of things

Unknown Speaker  34:35  
always

Speaker 5  34:43  
or ever good. Right? Not on your product. We have all of the pictures taken of all the whiteboards we did we are sans paper at the start so we're doing a lot of reason or focused on is talking about the architecture itself. So we started a little bit more traditionally right we got into goals guiding principles. And so we wanted to make sure that we created a framework or framework is actually everything. Sorry.

Unknown Speaker  35:14  
At any rate, and if you want to speak just

Unknown Speaker  35:18  
clip it on. It's perfect. Hello.

Speaker 5  35:25  
We started to goals be added in the use cases underneath them getting down to architectural value principles, quality attributes and tactics. We took kind of a traditional perspective on it, because we need to have those guiding principles especially we'd have to be able to ask the questions about the design decisions that we're making and be able to track back whether or not biblically yes or no now shadow shot nine metrics. One pillar in the hallway goals down to the into our tactics and design principles on the other side. So when I just start there and just say, this is our, our framing for the rest of our conversation. I'm actually going to ask Shannon to talk about goals because we had an interesting discussion and debate because our worlds collided a bit because Robin and I live a lot in the government space. So we're starting to talk about revenue, like not so much in the government space I'm talking about. Or if I'm talking about different types of workers software doesn't apply in the same way so

Unknown Speaker  36:31  
I'll keep going.

Unknown Speaker  36:33  
The trade off. So

Speaker 5  36:35  
business we thought we'd define business and government differently, only to start to see if we could kind of merge our worlds. We ended up there are two things that are unique to industry that are unique to commercial that don't seem to apply in the government space as margin revenue generation. They don't care about generating revenue. Sometimes they care about saving money, but they're not looking but goal of government is not to generate revenue. But resiliency matters to everybody and mission context. matters to everybody. So I'm the DoD space and non defense side of the house or worried about fishing context is delivering value to the mission. So this is where we ended up with our goals, four goals to apply to DOD all of them apply to business. I shared with the patina, a phrase that we've been using a lot in the DoD space right now is as commercial as possible, as military as necessary. That's getting after the mentality that we need to not treat everyone. There especially business initial context. I want to talk about the use case because you really get a good description, and I'll be glad to hold your laptop was trying to get

Speaker 9  38:00  
better. And I'm still on my various use cases, starting from business use cases market drivers. Decision Support being one of those essentially AI is being picked out to help with decision support which is going to allow us to do more with less current state r&d So if you're trying to

Speaker 9  38:58  
And then finally, simplification and accessibility. There are four major market drivers I've heard so far are the only ones but when you think about what's happening with AI, these particular four actually pretty well suited to what business and government that

Speaker 5  39:18  
was an important point. Stations little side hustle here about ageing workforce. Getting into the augmentation case for very specific talking about augmentation, number fixing, even though it gets into revenue generation and origin, right, fewer people higher revenue and all kinds of things over there. Well, we understand that that's an important part of the conversation. We actually came back down into these business cases. And ageing workforce is a dramatic reason why we need to be able to augment

Speaker 2  39:49  
it in the government space. It's a very, very specific problem, meaning there's no middle, right? So if you were to look at the Department of Defense, one of the things that you've seen is that 50% of the workforce or more courts just come up to speed in two or three years on out of college. So something like this could really, really push the DVDs.

Speaker 8  40:36  
In general training is a use case, right? It could be for even a new domain or even a new company or anything. And governance is actually another use case that we've talked about it. Maybe that's decision support some

Speaker 9  40:58  
can be or productivity, if you think about it, you're still like in governance, you're still falling on productivity from Congress, probably. But yeah.

Speaker 5  41:11  
I think there's a I think if we were to take everything we're talking about today, from across all groups, I think you'll see a pretty incredible, because there's so many connected, yeah. The four buckets that we just saw a conversation. I'm not gonna let Channing too far away, because she's going to describe, but I'm gonna take you down here and just

Unknown Speaker  41:34  
show you the first sentence.

Speaker 5  41:37  
back and out the door. We did want to talk about personas, we went and we were table Topping our assumptions and our conversation points as we went through. So we came up with these six, we actually argued a little bit about who's a user and who's not user potential became after customers, workforce, researchers, auditors, and we included in that oversight. The reason for oversight included auditors said, again, Robin and I are coming out of government perspective, there's a whole oversight aspect, that doesn't fall into some of the normal business, and normal normal business process, third party and adversary and we could all agree on those. So those are the personas. And we want to be able to use those across the board every time we're coming up with new ideas around this, to be able to come back to those personas and tabletop against them to make sure that they're that we pull up against it. So we started down into goals. And let's talk about metrics. Let's start from bowl based metrics. The very top is metrics or sliding scale Anis. And Shannon actually has some really smart thought leadership in this space. And she has a framework that she calls Raven, something that she's written about, I could argue for and against it. And I want us to have that good conversation. It's over here on here. Now we will see our ad and have her talk to it. And let's debate.

Speaker 9  43:01  
it. So one of the things that happened in the DevOps space are just an observation. We sort of didn't start out with measurement, we didn't start out with goals. And effectively that has led us down out how implementation and a variety of sort of the things that we do every day. And that's sort of a bottoms up implementation with metrics, there are three types with some categories we can put in where we're actually trying to get to a software trustful

Speaker 9  43:32  
where you basically include value and the things that you have to do for resilience or like reliability or abilities all into one low. So I call it Ray, its resilience, adoption, velocity and errors. And you can break down metrics into strategic tactical and operational metrics. And we talked about maybe even building a grid with arrays where we have the categories and then you can take the strategic cut the tactical, operational cutting process metrics to make them consumable for folks that would be adopting modeling. I wrote a piece before I got here that's what I wanted to be able to talk to some metrics that are useful and so I have a lot of departments

Speaker 5  44:16  
want to make sure it does anybody. Anybody have any issues with this concept of resiliency, adoption, philosophy and errors is kind of a framing construct, because it took me a little bit to get my brain around

Speaker 9  44:28  
It doesn't offset Dora, it actually includes Dora. It includes space and includes some of the other ones out there. It's just categories. So you could bring a resilience metric to this framework and add it to it, you could bring a velocity metric that you care about and put it into this. And actually leverage what you need to do is pillars effectively, you have to have one metric per pillar to be able to do some training. So like a great example is if you look at the Gartner Hype Cycle for AI as an example, you can see right now, there's a ton of trending towards adoption, how fast can we adopt, so you might be trading off your resilience for adoption, and you might be focused on velocity. So in the article I talk a little bit about the industry right now is really chasing the sexy metrics adoption of velocity, which is actually boosting revenues, at the expense of resilience. And Eric, so you're saying hallucinations are seeing. So it's, it's interesting, because I've come up with this balancing mechanism between them for trade off value. And I've been using it for years. And it's just an interesting thing, and

Speaker 5  45:35  
also going to resilience. When I hear resiliency, I'm automatically over here thinking about presenting supranational. And this is inclusive of that. But it also includes a whole lot of other aspects.

Speaker 9  45:47  
Yeah, so what you're looking to do with resilience is actually build business resilience. And business resilience is inclusive of technology, resilience, or product resilient to all the resilience factors. When you don't focus on it, you're gonna have long term business impact, or in this case, an organizational impact. I really don't want an organizational impact in my lifetime. And so when you start looking at the measures, you could be playing with this, you need to be focused, because there are actually 1000s of metrics that are out there in the industry. Why do you talk to people about what they measure why they measure it, and actually, it's cultural, it's got a value proposition to it's how people focus and what they do to build out their goals and their OKRs. And so there's a lot of science. I also mentioned type, so strategy, tactical, and operational, we pulled a few of the metrics that I had a good interest in interesting metric was actually if you wanted to see what's happening in terms of trade offs right now in AI, folks are actually going to end up with more legal cases over time where they're going to have to with things like they pay for their data, or they've got hallucinations or things that are happening when they left. The other two pillars are basically at the mercy of adoption of velocity. So you can you can almost look at the industry right now and see who's sort of playing which levers through a pillar situation.

Speaker 5  47:17  
Sir, we only put one or two in each category, because we know there are plenty more we'll add those and we would just want to make sure that this tells a good story and the closer you are to

John Willis  47:28  
coming on the journey with us from a story perspective. Trisha, what was your sort of what was your block that you should do? You could argue for because

Speaker 5  47:38  
I normally think of me as as all of these as falling into software quality attributes, so I take them down.

Speaker 9  47:54  
I think of these

Speaker 5  47:59  
velocity to me mentally, I started to think about them in terms of a team velocity and burndown. So that's where my mind naturally went errors. I think of that as defects escapes, and other things that are more operational in nature. Resiliency is an entire world when it comes to a lot of hardware that we deal with, but your entire test is built around getting after the hardware and software and intersection resiliency. So that's why

John Willis  48:25  
Yeah, it's interesting, because I didn't I didn't because you know, I got in on this early obviously, I'm a big fan. shed. But what's interesting is you say that right? Like resiliency that is in our industry right now you got so the Dr. Woods all spa version of that you have like sort of another offshoot and then you got software resilience. And so that so that's an interesting thing right there, and then velocity, you can get into sort of interesting, you know, sort of Toyota lien velocity diversion versus say, Don Reinertsen, and I was pronounce his last name wrong, but flow velocity. And so yeah, it's interesting. I hadn't really thought of this as the possibility of overloading

Speaker 9  49:08  
a trade off framework. So if you think about metrics and pillars and how you want to do what you're doing, this is actually built around the notion of trust.

John Willis  49:17  
Yeah, no, I get it. I'm just thinking now a lot about like, why might some people might get blocked on this, because the overloading of that view but but terms like you said this early this morning that's our plague anyway in the industry

Unknown Speaker  49:31  
We just need to call those out.

Unknown Speaker  49:34  
Yeah.

Unknown Speaker  49:37  
Yeah, I think that's a compliment but

Unknown Speaker  49:39  
we're going to have your your articulate about it.

John Willis  49:41  
That's the key point, right? Because like, I think it'd be really clear like, Hey, you might have a definition of resilience. But when we're talking about this is what the net doesn't like people get. Okay, stop where I'm at. I'm not going down this. business,

Speaker 10  49:59  
do I replace the word software business? I had a different connotation for all the same things, you could kind of keep going deeper to get to the software, right, totally. But anyway, I really liked that.

Speaker 5  50:16  
Yeah. We were going through vision mission values. goals and metrics. We're making sure that we're dotting all the i's and cross all the t's

Unknown Speaker  50:32  
For their work.

Speaker 5  50:34  
So we want to make sure that we're dotting all the i's, crossing all the T's when it comes to addressing business, we do have to have vision mission values, getting into go for it, these are the things that generally admitted business 30 said, some government agencies, not all definitely not all, they're not set down here. So we got into guiding principles. And they're pretty, pretty vague about this, your guiding principles are those things that thou shalt any kind of solution center, any kind of ecosystem that you're putting into place, we realize that there are really three for now. But that once you've taken that into your context, where we have context that once you take it back into your context, you're naturally going to have additional guiding principles. So we couldn't create one set to rule them all. So we decided on the top three, we talked about data quality, boy, that became something interesting and we're gonna have to have a sidebar with Shannon, teach us about data quality. And I mean, that was strong, was a really awesome place

Speaker 5  51:44  
ship right, that gets into a whole lot. There's a lot that is that we captured into words. A system doesn't mean done. This was a great conversation. Actually, one of you talked about this dish you came up with that term and love it.

Unknown Speaker  51:58  
I wasn't the one who came up with it, I'm gonna give credit.

Speaker 10  52:05  
Yeah, I think that's one transformational aspect of the car is how it's going to change are many organizations that use assistance for

Speaker 10  52:17  
the long running? hardware. So once somebody is finished with the task, isn't correct. Or something else to do is to go into the design thinking, building.

John Willis  52:45  
It's kind of funny, like, like the whole DevOps thing. It's never done or right like in this world, it's like, by definition, never again, because it's constantly hard as well. This

Speaker 9  52:56  
particular principle that I brought up when we were kind of debating it was when you have a principle that goes against a market or a market driver, you end up with a conflict and quite often, so it needs to stay but we have a pretty interesting conflict with how people are thinking about buying assistive technology. And yet it has so many Arab nations and so I think people are studying the technology just too far. I love the principles that we need to think about how we found the market driver, right, because I think that's where it's at.

Speaker 5  53:34  
Right? We're not gonna we're not gonna replace people right now. We are trying to get productivity, right. Increased productivity. So what. And then security, we haven't discussed security by design, security and security and the lowest levels. Those became our guiding principles. It took us into quality attributes. You guys all know, the delegates and we could go way, way down, but your ability is to it, to a certain extent, are driven by your specific mission and business goals. So we couldn't get too far down into them, but we decided to pull out the ones we felt were most relevant that we think will apply all the time. testability reliability, security,

Unknown Speaker  54:13  
yes. To availability, including getting after

Speaker 5  54:17  
the accuracy of the models, but also the accuracy of the information that the people are putting into bidirectionality to performance and maintainability that's something that we do talking about with all of this right now. And we know that the algorithms are tremendous to stand up in first place, the ongoing training, the maintenance

John Willis  54:37  
is tremendous. Despite the it was I loved the way yesterday, Josie would talk about with code samples of human in the loop, and I just thought was

Speaker 9  54:45  
interesting, in fact that he says that, I think there's something sort of really nice about feeling not that, like, we think we're gonna get replaced in the board. But but the point is, to your point, like, the humans got to be in the loop. Like there's all sorts of things and like, always sort of reminder. Yeah, that the human is going to go over time with the LLM, the expertise that we all learn and continue to reply rate to recall models. That will be interesting. But my belief is, is eventually what we're all arguing about. Why and how do you verify that those models

Speaker 11  55:24  
but there's interesting thing that I noticed in people when they first use chat TPT and also when they first start trying to build actual production, like product features of generative AI, the chat GPT they treated like it's Google. It's like, what's the most succinct single statement I can make? And then I'm gonna get something profound gotta come back, and then they go, Oh, wasn't that good? Or oh, wow, that was amazing. But they don't think of it as I think it was like a one shot sort of deal.

Speaker 9  55:50  
And that's because it kind of single blocks on the page. didn't

Speaker 11  55:53  
What is the chat, right? You don't think of it as a conversation, right? And likewise, you noticed when like, people are starting to first get the idea and build it into actual production, production products? There is the problem, the same thing, which is like, Oh, what is the magic sparkle button, you get some magic sparkle button? Where's the profound thing that's going to come back versus like we think about as an agents or personas or you know, functions, you wouldn't tell somebody? Hey, let me give you the shortest cryptic sentence I possibly can pass it to you. We'll sit back and see what frowning you do. You'd say a few things. You'd ask a few things you refine in on it, you asked me what I think you'd suggest other things to do. And I think like when you see the people that have really mastered the chat, GPT way of working, that's how they work. And then I think you've seen that also. Now when it comes to products, when figured out is like Well, other than a chat interface. How do you do that? How do you kind of have a conversational interaction with a product to guide it down that? So yeah, it's I don't know. It's that same mindset and interesting mindset shift that takes people out of maybe the Google.

Speaker 8  56:58  
It's hard. We're all sorry that one of the challenges we have is actually customers keep putting just keywords like to do Google searches. Yeah. And so they're not unleashing the potential. But it's always like, sometimes we feel we need to check whether that was a Keeper's query, and then we'll generate the question. It was actually meant to the model to go from there. But I also wanted to comment on the humans in the loop, right? So we found, yes, if you ask, let's say you have to give permissions to 20,000 documents, the LLM is suggesting whatever needs to be done. You see, like, it's the trade off? Am I going to press 20,000? times yes, or no? So you verify sample, you say yes, and then go. And after? Why used to say go? Because, you know, you start trusting the system? And that's kind of like, yeah, people want to have the human in a loop. But as soon as you start showing some things that they started trusting, they're very fast at giving up. And it goes, and it's the same thing with the pipeline. I keep telling like, people trust the pipeline, because most of the time it's working. So it builds up that trust but like in essence, it could fail as many times as right. That's why it was designed. So yeah, they're very easy to give up that human evolution.

Speaker 9  58:22  
Well, I also think it's important for us to realize that these models are really coming apart, and that the technology is really not ready for what was released to the world. So if you think about it, like, you're going to use chat GPT as part of your assistive technology for dialoguing with a customer who doesn't know how to talk to a GBT, right? And so either we fix the technology or we fix the humans, but something has to happen to balance out. the illness,

Speaker 8  58:51  
But the point is that if you have to do you have automation, that they need to do a lot? It just, it's very tedious for them to say yes, yes, yes. That's kind of like you know, after a while, there are

Speaker 9  59:05  
a bunch of questions so I can do the right job, right. And so I think, again, it goes back to is the technology really ready for all it becomes like,

Speaker 8  59:15  
is it failsafe, Can I roll back? Can I kind of like verify this, you'll have like getting tests, and kind of that that complements that very human in the loop? Even if, you know, we'd love to verify

Speaker 9  59:27  
and that's why we wanted to spend some time on the whole Ultimate metrics that are really the basis for the conversation? Well, we probably have a good goal, but we haven't really reached it yet for it to be out there and doing well. So you're gonna have a ton of errors. And so if you look at if we were to start benchmarking metrics for all of these AI implementations, it'd be quite frankly, was interesting

Speaker 5  59:51  
throughout the entire day. How much of this group so far has been able to project where this is going to go? And everyone's while you have to take pause and say, technology's not there yet, to your point. Right now, there's still humans crafting the prompts. We're still engineering prompts. Yes, there are frameworks now too, that are very fledgling that are helping us to create better prompts. But it's still we're not

Unknown Speaker  1:00:17  
doing it to other prompts. on it.

Speaker 5  1:00:19  
But but it's not yet feeding, right? We're not directly influencing and having one framework and some other framework, right at that precipice right now. But that got us to the final thing that we talked about when we were tactics are the actual getting the trade offs for those quality FX misses. It's gonna depend on the business, we really decided to pause, pause at that point, because it doesn't make sense until we kind of break off into domain areas, whether it's healthcare, medical, DOD, even tech.

Speaker 12  1:00:53  
All right. So good group for us. you

Unknown Speaker  1:01:09  
fell on the ground. Here

Speaker 11  1:01:17  
Oh, yes. So we'll just talk about what we'll get back. So we're all on one second.

Speaker 11  1:01:52  
And then I got The data project, but we were trying to not end up in this theory land, but actually started to think about use cases, and then figure out okay, well, what the data would be for that use case. And we want to stay away from kind of the classic sort of data science, you know, create some insight out of a lake of data type type things. We thought about what what are some use cases? And, you know, we're talking about humans and how humans work how humans think and you know, retrieving that, can we come up with a general pattern of work, right? And then from that pattern of work? Can we look for different use cases where that maps to? If so, then let's go figure out how to get that data. How do you store that data? How do you work on that data? Right, so we talked about kind of the work inside any company, you know, whether you're tech strong, or whether you're, you know, General Electric, right is or MDC, right, it's gonna be the same type of work, where you know, how decisions are made, right? You basically have this stack of information that comes up through the organization, right starts with the people that actually interact with the world, right? And they have their telemetry, they have their interactions with folks, there's transactions, whatever, whatever it is, right? There's some interaction with the world. There's some management layer above them, who takes data and narrative, right synthesizes some analysis of that. There's managers above them take that data and narrative and synthesize analysis on top of that, that goes all the way up to the CEO and the CEO has to work CFO LLC, let's all C suite has to make some set of decisions, right? And then that strategy has to then then the paths down through that through that chain. Right? And so strategy goals go upward results come downward. That's going to be strategy goals go downward results come upward. And, you know, this problem. This is it's super long lifetimes, right? Like, what's actually happening here. There's layers of compounding, you know, full progress there. Right? Just it's just, you know, whether it's intentional or not, there's very low competence. And there's also very low confidence in what's coming up to the executive layer. And there's very low confidence that they have any control over what's actually going on down through the layer, right. And so, your vision used to kind of think of like, well, we use data to solve this problem. It's the classic like, a cup of tea. science, sort of magic bullet writers big lake of data and we're going to create some really match a query that's going to create some profound information that's going to be in a highly static dashboard, right? And it never hits the mark, right? Always have the same problem. And we're not getting better at understanding kind of what's coming up the chain and what's going what's going to happen, right. And so, the new vision that we have is like, well, you know, how would human beings do it? Right? Well, again, it's it's data, collecting data and narrative about what's going on at the ground level, synthesizing that data narrative at the next level, sort of

Unknown Speaker  1:05:07  
as you as you

Speaker 11  1:05:09  
go, and you know, whether that's done is sort of shadow kind of function, right? Or it's done as an overlay, as personnel really, really important. Important is at the top, can we have more accurate analysis of what's going on and can we turn that crank a whole lot? A whole lot faster, right? So instead of doing like an A sales example, like a quarterly pipeline review, instead of doing it monthly to see are we on track? What happened every day? Right? What if like we had, you know, agents at that ground level that were saying that we're curing the data saying, Hey, we really have all the data for this account. We really, look what happened that last call and we actually watching the news what's going on these customers and judging whether or not this is going to happen or not right? And then on top of that are

Unknown Speaker  1:06:29  
people work in the world. Right? So that's

Speaker 11  1:06:31  
another one out there, which is decision support that we ended up dealing with, apparently, you're talking about fires support. Now, we're talking about sensors to shooters, and I'm talking about that kind of decision support as well. Some of its near real time, some of it, it has much longer latency for it, but it follows that similar kind of pattern. Yeah, that's right. So I think this is just like getting human beings together to do some kind of, you know, large operations operations, right, because business operations work, or managing battlefield or managing company management. Right? Probably, it's all zoning. So that was the idea. So the idea is we thought that was a pretty good model. So

Speaker 4  1:07:15  
yeah, the next sketch did you want to add there

Speaker 13  1:07:18  
I love it. Yeah. Thanks for commenting on this one is this sort of synthesizing information and passing it up? It's got tons of bias and a lot of inaccuracy because it's people's interpretation, their their scope of what they can see without seeing things that they can't see. So if you noticed sources, not just what people put into action reports that they're dealing with customer sales call or their customer service call or something. But also taking zoom information, taking emails directly, combining all that together, and you can get a less filtered or less biased kind of information. The other thing we thought we should do is take it into another step is, let's say you have a sales plan for specific candidates in AWS. You have a plan, how many deals should think you're going to be doing what's in the pipeline? What's your strategy for approaching that account? And you have a lot of interactions from your sales team and customer support vendors that are working with them. But there's also outside factors to get gets on CNBC and says something interesting, we've not seen that there are conversations with the cutback customer, the new CFO, the entire focus is getting disruption change, there can be outside, not just your people's contact with that organization. But that data can also stimulate research as sort of a what if analysis is this event is this is this speech that somebody gave somebody that might change or impact ourselves strategy for that account. And we might send ourselves people out to let's go talk to the customer ask him what's happening with this, get an update, we like to get more real time. So instead of waiting for a month, or two or three, to take action on something, you might actually have yourselves forced to do something different. Or a subset of the test or whatever. We're not trying to react to every single state of the world, but try to match it against your data and your plans for that customer. They were talking about the media example. There's all these things happening in media Extra wherever we want to generate articles and blogs and conversations about all the sudden kind of filtering it up to like analysts reports, that's something that I might work on. But there's also talent and I just did this roundtable this morning. He has he said the chat that was going on was on fire because we were talking about AI and software development. Immediately after that, we should be talking about one of the next six roundtables we're going to take data like that, like you might not know that we were on that particular roundtable salute Good day to happening in your organization that might change our direction in those dimensions.

Speaker 4  1:10:17  
Yeah, I think just to finish up right, like what are our next steps, right and it's really looking at how do we deal with data captured organized, dry cleaning, and then how do we integrate that into like some of the other teams

Unknown Speaker  1:10:28  
you're talking about the pipeline's?

Unknown Speaker  1:10:49  
I am

Unknown Speaker  1:11:00  
All right, six were perfect sort of contract

Unknown Speaker  1:11:12  
cards,

Speaker 14  1:11:13  
I was just looking at the list of like I found a converter that would drop into flat tax filers and take textiles and drop it back into WPS and I was gonna handle that So probably

John Willis  1:11:24  
what do you need like a half an hour or something like that in the morning? You do sort of round up on the mapping and all that

Unknown Speaker  1:11:31  
yeah, I got really good sci fi

Unknown Speaker  1:11:40  
have been working on

Speaker 15  1:11:42  
like months to years like like some of these maps things that you saw one of those things often last year bear in mind

Unknown Speaker  1:11:57  
uses Matt

Unknown Speaker  1:12:08  
looking forward to it.

Unknown Speaker  1:12:14  
augmented cognition has been huge. nobody tying it into like the Simon's workplace stuff I do maps

Speaker 15  1:12:29  
and there's other side of this Oh wow. There's other horizontal flow like you can do certain things. Gentlemen demand for your process

Speaker 14  1:12:46  
early on Okay. From a complex systems perspective to black with black swan firm tendencies have two possible things that should never happen because He

Unknown Speaker  1:13:15  
drew the model choose

Speaker 7  1:13:18  
with all the permutations to find where they align or

Speaker 16  1:13:24  
I'm doing a platform engineering value stream that allows

Unknown Speaker  1:13:38  
associated technical ban which

Unknown Speaker  1:13:43  
is really focused on Compute like

Speaker 15  1:13:52  
once those got taken care of with like horizontal scale

Speaker 14  1:13:57  
we the bottleneck is the data that leads straight into AI elections because they have started

Unknown Speaker  1:14:09  
situations.

Speaker 14  1:14:10  
given that is where the four senators present 20 Sorry, how do we pull that into Hello justice diagram, right fit into the broader half

Unknown Speaker  1:14:29  
kind of like, three worksheets.

Speaker 14  1:14:35  
like super encouraging day is when I was before the breakout session, backfiring, Josias bad fit my chair, just my mapping convention together. But observability is frontline on their operation, y'all like it like that service. Really? That's like wise. Let me sit back and watch as a facilitator. I'm supposed to observe and I interject, okay. Governance did a pretty good job there. How I would read it, architecture or

Unknown Speaker  1:15:38  
architecture and data is expected. Yeah, so

Unknown Speaker  1:15:48  
stay tuned out,

Unknown Speaker  1:15:52  
Nicole. So that's sort of the compliment I'm going to bring

Unknown Speaker  1:16:01  
back in to how we should actually account for all these human factors

Unknown Speaker  1:16:12  
to capture that I

Unknown Speaker  1:16:19  
feel a good vibe rating

Speaker 14  1:16:23  
is nice. It's nice. It's like well that I expect to score better

Unknown Speaker  1:16:38  
then I guess

Unknown Speaker  1:16:41  
this was maybe the second half tomorrow you can grab

Unknown Speaker  1:16:47  
a beer over on the Saturday probably. Maybe

Speaker 15  1:16:58  
we get four fingers. Try to get even if I get some of your code running on your laptop, I will help you as much and just sort of playing operationalize some pretty good understanding of it.

Unknown Speaker  1:17:22  
Like, I haven't

Unknown Speaker  1:17:25  
I've been doing a sociologist.

Speaker 7  1:17:27  
backpack. I have a bunch of like, bite sized stuff. And then I got the white paper generator outputting, the PDF and summary sections. So it's all nasty shit, I need to refactor. But then I have a bunch of smaller samples that all work Give you all stuff

Speaker 14  1:17:50  
I've been like our internal hackathon thing at targeted.

Unknown Speaker  1:17:55  
I gotta find kind of cow.

Speaker 14  1:17:59  
Areas like this. An API account. I've been looking at Friday, GBT stuff my dad was like, I don't know that there's another implementation to

Speaker 7  1:18:27  
when he was using showing for air gaps implementation as well, three air gap and one not air graph, I think I'm not sure if he was running. He was running him locally, because he's doing showing CPU and IO and stuff. You cannot you can't you can like EPC air gap. And like how you're facing, you can post her

Unknown Speaker  1:18:45  
externally. And then

Unknown Speaker  1:18:48  
I think that h2o is effectively AI. That's the first time I saw it.

Speaker 7  1:18:56  
But like I got, I got it working on my laptop and allowed Lama to and stuff like that. So there's a number of implementations I've done. Researcher GPT there's a bunch of them. They're not nearly as performant as open AI

Unknown Speaker  1:19:20  
for the use cases,

Unknown Speaker  1:19:23  
like from a consulting perspective

Speaker 14  1:19:28  
like they're like We can do to change another thing that's like on top of using flashing AI techniques I'm saying how on top

Unknown Speaker  1:19:43  
of solar winds,

Unknown Speaker  1:19:45  
that have solar like

Unknown Speaker  1:19:48  
flares like I that one keeps getting service for

Unknown Speaker  1:19:52  
a good search engine.

Speaker 14  1:19:55  
memory so it's pretty quick and it does cinnamon management,

Speaker 7  1:20:00  
sir. Are you accessing that through like an adapter that a lady usually train example, but your business logic? Are you writing an adapter where Mike chenko? would go ahead and pull data out of that? Or are you doing it like, like, you can do it for Redis? For example, another database, you can use the vectors

Speaker 14  1:20:21  
that one the Classic Controller to experimental controls.

Unknown Speaker  1:20:26  
Okay, so you would just call an API or a

Unknown Speaker  1:20:35  
classifier and generate this? Like?

Speaker 15  1:20:39  
Do you end up with more mathematical out around grabbing a bottle. and it gets back to the whole like it's not going to be generated? Again, what's the operational risk? Here's a consulting perspective, they come in anyway,

Unknown Speaker  1:21:01  
I want to be generous

Unknown Speaker  1:21:05  
with the risk profile. Maybe

Speaker 15  1:21:13  
we're gonna get 75% of what you're looking for there, it's gonna cost you

Unknown Speaker  1:21:20  
it's not gonna go off the rails.

Speaker 15  1:21:22  
It's up in a month, and people already know how to drive versus like, we've got the ground you all the organizational capabilities.

Speaker 7  1:21:33  
Positioning more as like, are you positioning as somebody who's achievable?

Unknown Speaker  1:21:40  
measurable, achievable, realistic, and time bound? And then Gen AI as your small implementation? Learn? Like, side by side, are you?

Speaker 15  1:21:55  
that it's the question I'm playing? That's the question is where is that threshold differentiating I can make sense for you to invest in J. Gen AI, given your context and business and organizations. like yours need like 10s of millions

Speaker 7  1:22:21  
From a consulting perspective, is there an opportunity to deploy something that works today, which is a great consulting, engagement and, and put put put in the perfect concept, so you have the engagement nonproduction proven concept limited and limited in scope, and responsibility and risk, but allows you to continuously position yourself and bill

Unknown Speaker  1:22:43  
that's exactly

Unknown Speaker  1:22:46  
I can't say too much, but it's

Speaker 14  1:22:48  
part of our business strategy, that would be one RFP.

Unknown Speaker  1:22:57  
Which we can trace

Unknown Speaker  1:22:59  
back on that.

Unknown Speaker  1:23:02  
It is really

Speaker 14  1:23:09  
smart cover a lot of it and then we can keep iterating off of that as you move forward,

Speaker 7  1:23:14  
I assume in your own space, using a lot of generative AI to

Speaker 14  1:23:18  
chords, durations. And they, they want us to build one internally. Like the thing I brought up, how many times in Slack during the week.

Speaker 15  1:23:33  
We have a slider report saying and I don't know SharePoint is out not only should we have

Unknown Speaker  1:23:47  
to be able to generate new clients

John Willis  1:23:48  
Have you tried? Literally Oh, yeah. No, absolutely.

Speaker 14  1:23:55  
We're like mice. They're like, way too excited for me to get back.

Unknown Speaker  1:24:02  
It's it's a

Speaker 15  1:24:04  
game changer. And it's kind of my point. I couldn't I couldn't run an index, search, search index off those sources with solar. API access maybe we can

Unknown Speaker  1:24:25  
build them before weekend.

Speaker 14  1:24:27  
And it's not gonna it's not gonna generate like a new slide deck for us.

Speaker 15  1:24:35  
But it's gonna give like, it's gonna pull all those materials into one place. The digital console onto the train. Oh yeah. The point of all right now we've got degenerative AI.

Unknown Speaker  1:24:53  
But what's the quality on it? And what was the cloud? We're actually worth the money.

Speaker 7  1:25:01  
I think within the context of this white paper for Android, right, and there's a bunch of questions like What's my what's my value, all the standard, standard standard questions and getting them investment built up the white paper. And then when you have a white paper, that's fine. My interviews, my data wherever that ingest that goes back to the QA a

Unknown Speaker  1:25:27  
bunch of questions that come along With that, if you have a rough white paper you

Speaker 7  1:25:36  
put that through and evaluate taking it put it into a PDF, throw it into two charts by PDF in chat GPT then do an evaluation flow form right up there. Okay, now take this break it into break break thinking concepts into power efficient PowerPoint, conversation, boom, I have this headings sub headings, I can take that into do that into Google Slides or whatever you want to do. I have taken into PowerPoint using designer pretty pretty, pretty pretty working workflow. My goal is to bring it as much as I can.

Speaker 14  1:26:17  
to lane change. I mean, I've generated obviously like events on the fly like they're technically XML Yeah. The Queen's code because but like, I'd like to kind of old school over

Speaker 7  1:26:38  
Oh, yeah, that's all right. All tucked back. Down right, go meeting up for dinner.

Unknown Speaker  1:26:57  
You

