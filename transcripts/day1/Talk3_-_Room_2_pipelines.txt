Unknown Speaker  0:00  
To,

Speaker 1  0:01  
to build code is not seven different processes. It's one process. Yeah, like what you're saying for your whole organization, but, you know, it's just like, what I just realized is like people don't necessarily understand

Colin McNamara  0:19  
the context of in the context of AI. You know, if your business partner right, she'll be like, Oh, I can't wait so AI can do this. Right. And I've had to one get past my frustration, like nothing in technology is just just in should or weird words. Right? But then like, Okay, well, what's the best way of it was accomplishing this while maintaining empathy and being approachable and not being asked? And is in to be able to demonstrate the individual components and it seems like you know, that that that the blindness of AI can do this okay. Well, what can I do can we take it can we look at information can we apply cognition to logic of information can we bring with Texas it's a little bit easier sensors. This is one meter. Can we bring this in and out of like a transformer transformers model or something to create create images out of it? Okay, now, breaking it up. Can we bring this into a pipeline? I feel like there's a set of demonstrations from a solutions architect. Maybe there's a there's a set of demos of individual components that are small references, that maybe that's something and I don't know what the output of our work right here today is. But what like what's the smallest? What are the smallest bite size digestible ways that we can demonstrate the multiple areas that comprise this overall flow that you're talking about, you know, in a way that can least route someone's perception and understanding for that larger conversation?

Speaker 1  2:05  
Is it really bad? It's like, you know, like, people will get this like, they're like, Oh, we want you to this AI and they just really think it's going to show up and magically solve all of their things. And, or it's like, we want generative AI and they don't realize, take me five or 10 years to implement because they don't have all the prerequisites, right. Like it's, it's, it leads to more frustration and more work overall, just like,

Unknown Speaker  2:33  
maybe like I'll say two A's.

Unknown Speaker  2:35  
Yeah, exactly. Not necessarily. Is that good or bad?

Unknown Speaker  2:41  
Right. So where are the four of us? Yes. So I miss like a large sheet of paper.

Speaker 2  2:50  
Well, let's do that. We're gonna mark on a green wall. Okay, so we do need a piece of paper or do we have more

Speaker 3  3:09  
digital stuff. I know I was like, every time I need a sheet of paper, I'm like, where did I add like a stack of legal pads? somewhere and I always move them and forget where they're at. And then I ended up taking a picture with the camera and it's in my iPhone photos. Anyway,

Speaker 1  3:33  
there's something better about just writing at least for me, like my whole desk was like all kinds of sheets of papers, but it's doesn't like it's worse if I take a picture of it actually. Because then I think I remember what what I put down but then I don't, you know, like, I don't actually remember

Speaker 3  3:57  
if I'm on the road and I don't have my big screen to blow it up. Then it's just sort of worthless to me. But if I had my big screen, it's pretty possible. And sometimes then I print it back print out the picture on my printer again, but

Unknown Speaker  4:13  
I don't he's gonna take so we had pipeline tools, agents.

Unknown Speaker  4:25  
What were they talking about the agents Oh, wow, you're

Speaker 2  4:28  
right on the back of this. I have someone getting some tape. There's also a window

Unknown Speaker  4:34  
you can put it here

Unknown Speaker  4:51  
there we go.

Colin McNamara  4:57  
Yeah, save for anything. We can take this to a wall. Oh, perfect. Need help we need we just wanted to create something to write on the whiteboard trying to get away or do you want this? While they're gonna use their

Unknown Speaker  5:13  
new one, and we'll see if we have

Unknown Speaker  5:16  
a year or two. We'll get this working. There we go. Race. No,

Speaker 4  5:24  
I mean, if you want to, I can get us some gaffers tape. You can hang it up over here if you want to. All right, we on the green wall on a white wall around just smart one. I'm just super gumming. Logistics logistics

Speaker 5  5:56  
I remember trying to reverse engineer the Bluetooth or Wi Fi and the hours I spent on that. I could use that on and off.

Unknown Speaker  6:13  
You guys are the tallest we got tall guy so over here on this Walker,

Unknown Speaker  6:17  
I'm not and you might need them to save it That's

Unknown Speaker  6:27  
right there

Unknown Speaker  6:37  
you're doing the making

Unknown Speaker  6:39  
the behind the scenes really really

Unknown Speaker  6:51  
nice way I think it's more of a permanent

Colin McNamara  7:01  
side it's just has a calendar on it really straight critic

Unknown Speaker  7:12  
Yeah, when I leave the room you're not gonna want to know what I say. But I love all you have to say.

John Willis  7:20  
I want to remind you of these great artifacts because otherwise you'll just come back. Remember what we did

Speaker 2  7:24  
probably lots of useful, useful pages. and sticky ones. There was like an Office Depot

John Willis  7:33  
near okay Center runner. every day. Right now just yeah. a list. of things that we need from Morrow. back. Do you have a list now? I can probably get

Speaker 2  7:44  
by. Yeah. What do you do? Yeah, you got those posted easel paper. Yeah, right on and then you pull them up. Okay, a flip board He wants your tablet

John Willis  7:57  
or anything else because that's what you need. So when did we get somebody to go out and get?

Colin McNamara  8:04  
so post it has it's a it's like a three foot by two foot kind of like peel off paper thing we're gonna draw on them and then you stick them on the wall tear off. It's like a giant Post Office Depot flip chart

Speaker 5  8:30  
So pipelines are to what we start pipeline.

Speaker 3  8:36  
So just to be clear, the data pipeline that's going into the model the pipeline for what's coming out, like what's the pipeline? All that's like like, that's, that's just my question. You want to start like, like, let's just say data source for you. training, fine tuning, or I would even though the user pipeline, I think my view I

Speaker 2  9:05  
think in the simplest terms I have I don't want to waste the paper consists of a finite thing, right? So somewhere out here, I have a data source

Colin McNamara  9:23  
right now of this data source might be PDF might be HTML might be a JIRA database, right. So if I have like HTML juror dB, which is really just sequel I have my husband Mark down, switch recently just more and more right. So, in these data sources, I then have a load operation into my back my vector store like the kind of first basic operation

Unknown Speaker  10:14  
in that load I have

Colin McNamara  10:15  
my my texts with chunking. I have my metadata duration and out on the other side of the spectrum EV, I have my weakest link check some messing with there's many different ways of doing this. But I have a link chain code which

nests in here I have might have my prompt templating prop templates, my conversation routers my my LM craziness right where this popping into my L lens, which I might have on the top of this like a war adapter. Then coming out of this I have some sort of outlet or just in the most simplistic format right this output in my view reflection back to the terminal and might be able to switch your presentation layer to a streaming interface might be to an output document. Document for application by looking at this model, add like horizontal view in a release in that's in depth that that right there is a pipeline that we do in our notebooks with this look at Jenkins with this look good and whatever I don't know that's kind of what I view is like the most simplistic pipeline and then it gets more complicated from there. And I

Speaker 3  11:53  
so what about streaming data as well? Or do you say stream everything into the data source could be streaming it could be okay, I'm just I'm just asking like, it looks like a data lake. So I'm like implying data lake but I'm wondering, does it matter if it streams into the logs? Logs? Yeah, that's that's sort of what I'm thinking like Kafka or security logs stuff like

Colin McNamara  12:21  
absolutely. I mean it could be a maybe I'm using worsted video. In video orange stuck to a webcam, the student object recognition is counting the birds and the cats that come to my theater. Okay, right. It could be anything that starts coming into breaking this down applying a logic to it, and then fly flying out but what's like the most simplistic form that we can use for our examples? Yeah.

Unknown Speaker  12:48  
He's gonna go to obviously a metaphor. Yeah, yeah, exactly. Do you need this if you notice posting it's fine. It's like sticky past

Colin McNamara  12:57  
is really big. It's like, three or four foot by two foot thing. It has you rip it from a lot on the backbone. They have like a sticky back. That's the guy where they're, they're like on the back end, near the whiteboard. There's where the easels are the swords where they have a piece of paper and a flip chart. Yeah, they have these flip charts completely rip it off and stick it to the wall.

Speaker 3  13:21  
It's just a great big sticky note. I don't have any at this particular store and there's something else. Yeah, just just paper because that would be me. Yeah,

Speaker 2  13:31  
yeah. Okay. Do either one. I really appreciate your help. If I have any questions I'll call Andrew. Yep.

Speaker 5  13:38  
Okay. No. So I guess for if you look at the arrows, but it's up, sequential, I think so. You might have this is this is generating almost like derivatives, which is the embedding. Yeah. So and all kind of this is data, and kind of all kind of this I would say, put them under the umbrella of code. So you're basically taking a snapshot of certain data, but maybe you're feeding in data directly without the fact that database is kind of like a miniature become more of, let's say, models. So if I were to look at like, putting something in production would be I need to do some pipeline on this part, or maybe have to do some pipeline on generating whatever in the pipeline. With the past and more hands on, and I might have to do some transformations and some some kind of pipeline on getting this into data leak or kind of ranking. So So and then they all have in a way their versioning where you can say you know when when I'm promoting this, like from one environment to the other. Is this coming from staging? Is this just representative? Is this kind of the prompt that we're using? Are we using a production data? And the same thing like the first one of them well, that we haven't promoted yet?

Colin McNamara  15:24  
Oh, absolutely, like, say, like contradicting I'm trying to kind of like get oh, yeah, I don't see this contradiction. You know, when when I hear so if we as you have each of the elements that can be deployed in version in the pipeline, so you have multiple levels of the business logic. If I think of like a business logic way, right, where my business logic is now in my prompt template, just like you're describing your use case. Right? Yeah, yeah. Okay. Well, I have, you know, a branch was not promoted in my pipeline and I'm, I'm trying different use different different crop templates to extract value out of my data to come up with my outcome in which you know, maybe measuring increase sales increase security, decrease operational risk or decrease odds you had a tray.

Speaker 3  16:07  
So your code layer there with Lang chain. It's almost like today we use line chain because there's not a lot of tooling there. But eventually there's going to be security combo, like it's going to be your LM middleware. Yes, right there. Is that that's the real thing. I think where the interesting stuff is gonna happen. The security people will say, Yeah, we're not going to you know, let this data pass we're going to be able to sideload in like these Laura packs that are you know, builds based on we've added we add a new system we add a new type of log. So how do we update the model and it's going to be a new we're going to add that Laura pack on top of the

Colin McNamara  16:49  
filter functionality performance. So you may you may be insert new new new chains which which are chained which are chains of prompts effectively together and business logic and then through and through observation of your pipeline see performance or effectively performance and or cost impact and choose to take the decision to train set and train set chains down into an adapter in front of them.

Unknown Speaker  17:11  
Okay, okay,

Speaker 1  17:12  
that's my thoughts go away, assuming that at the very beginning at the data layer, like you're starting with the same set of data for at briefing because like how, like, I mean, what do you want to write because like, like right now like say like, if you do when you're doing QA like you don't at the beginning of the pipeline, you don't queue a rolling with real data,

Colin McNamara  17:37  
you expect the expressive data set so that you know when you're doing one variable at the same time,

Speaker 1  17:41  
yeah, but as you get further down the pipeline, right, like let's say like, once you start doing performance tuning or something like in staging then the user the data set becomes both larger and more real life. So what I'm asking actually is like, are we using the same data of like, because will it make a difference in the model if we use the same data or we use it abstracted version like earlier on because that's the other thing, right? Like right now, if you think of sock to soft to sock to the airport security, right? Like, the developers don't have the same clearance as a person like IT operations, right so operations can look into the real database and PII data and other stuff. But what happens if that data is removed? Or

Colin McNamara  18:34  
I think one of the mentation that I've seen using the tools engineering script, Kitty, that manager

Unknown Speaker  18:42  
there's all kinds of stuff.

Colin McNamara  18:43  
Yeah, and so things like Link Smith, which is a debugging tool, I have a booth for everyone who want to use them. That basically turns the link chain code on a debugging mode and takes the every step of agent execution and outputs it to a tool that's parallel. So you know, I see different tooling, as in the pipelines, observing it being integrated into the into the workflow. And so one of the use cases is you can take the code that's executed, and create that a credit data set out of it, that can be referenced to revise again. And so in that case, the

Unknown Speaker  19:29  
back to the beginning of the mind, so

Unknown Speaker  19:32  
I think so I'm just learning. I just got it last week.

Speaker 3  19:35  
So that would be the backpropagation and testing through the model. Right?

Colin McNamara  19:41  
Well, I think one one area and then what our bill was talking about the two for testing for security stuff, right? So doing prompt injection, red teaming and stuff like that. So there's there's different things I posted in the Slack channel. Some examples of constitutional AI, which is basically a set of these we've defined a set of principles that you're that you're observing the logic to see whether it's executing on now, you know, the example I posted was inside this code, but that seems like a very logical step to have that as part of this that are executed to validate in both the development cycle and then I guess, in the overall in the overall application structure and production.

Speaker 3  20:25  
But you can do you think that's like initiated by the initial system prompt for the LLM? I mean, the Constitution is

Unknown Speaker  20:32  
actually using it in the chain

Speaker 3  20:34  
using the chain but model itself having its three robot law, is that what you're talking about, like

Colin McNamara  20:41  
None that I've seen it in the models, I'm sure smart people have implemented it. I've seen it in the business logic layer where they're actually using the LMS cognition capabilities to observe the inputs. And outputs to validate whether our matches are principles whether this is like PII data confidentiality, appropriate user role have role our back are some use cases I can think of.

Speaker 3  21:05  
But take a look at the leet system prompts for GPT 3.45 and four because that's where they put it in. Oh, okay. Okay, I just didn't prompt legal system prompting it was like yeah, just look for like that's the that's where they put it because that way it's it's can't be tampered with because say say everybody is developing on that. They can say this is my, you know, North Star of how we're going to keep it and it keeps that sort of how they're keeping certain. Like, you're not going to do anything harmful, all that kind of public good stuff.

Colin McNamara  21:43  
What's the nursing take on that? So in the intellectual interested in an example with the clap so you can run in and see yourself? I go through the example of showing basically, not authorized How do I kill a cat was was the example and it shows how open AI kills it. And then by importing the constitutional constitutional code, in your code, it throws a fall before it comes out. And then there's another example as it gets a little bit more matured and is able to catch up so you're not you're not even passing that up to the LM you're catching it as a business logic layer. So like I guess, depends on the depth on that. And

Unknown Speaker  22:23  
it's both definitely used, both have been used.

John Willis  22:25  
So this is interesting to see the room one right, the depth right now. So this part is so nice. So whatever we'll come up with here we can sort of cross connect.

Unknown Speaker  22:40  
That'd be that'd be University. Yeah.

John Willis  22:42  
What's the example I'll go

Colin McNamara  22:43  
back and tell them I'm using the constant set of questions Slack channel, decision discussion, they the constitutional principles inside of the main Chinko constitution. Ai example, basically is looking at the LM code, or the code that's executed prior to getting married LM validating, and Susan, she's safe secure.

John Willis  23:05  
I'm being useful.

Speaker 5  23:07  
Very coming back on to your question of no is this all the data so the way that I see this? So if I think about a dependency graph of you know, your code, right, and it has like, the library has the card and there's all like dependencies. For this, you have dependency of data, your problem, so they're all like, you know, they all have their own versioning Yeah. And then they all you all want to test them. And so if you want to test a and might select the sub sample of data from dependency, and you're kind of doing this, so they all have this notion, I think of versioning in like, you know, this is the sample data. And this is the version that I'm testing. So this becomes more like freshening at creating tests for each of them in combination. There's a lot of integration testing with decent documents. We're trying to test this, that problem with that model, so becomes like a lot of different variations, depending on how they're each progressing in their own pipeline and promotion, and going through and that brings up the fact that they're all like, typically in C ICD when you create an artifact, one is tested code, the problem, they're all getting promoted along the way. But because this is a combined stack, you can't just like promote one because you have to go back and version almost the stack. And we all start out. Like we're gonna do that separately, but in reality, what I'm seeing is there, upgrading a lot of them and then making a job. Or like they do it with more like feature flagging, promoting, like in Chinese there. And then once you have the deploy, it's like because you have the dependencies. It's has like this orchestrated deploy, firstly, do that and then do that. And then finally, they don't want to kind of start looking. And then you run the analytics on how they're each performing, and then how the whole kind of change

Colin McNamara  25:21  
so functional tests to be subcomponent integration, test the, the items necessary to deliver as a whole on the non functional tests to identify performance.

Speaker 5  25:33  
So all the same testing, but it's just you take a combination of this matches that and then you can go as far as before using that exploit or strategy with those kind of documents with that kind of it can go wild so but the reality you're settling on a few of them that you kind of see typically evolve.

Speaker 1  25:53  
I think you have to write otherwise, how are you going to make producible like right now, like, for CD, like the best way to do cd, that's why like Kubernetes, I think took off is like because you promote the container. Like it's the whole environment. They cannot promote this.

Speaker 5  26:12  
They kind of didn't do that and it didn't work but for the data we've always thought like, you know, that's kind of leaving their own college pipeline of promotion. Yeah, once that promoted, were promoted, the binary are different.

Colin McNamara  26:27  
So how does one thing that was interesting for me justice conversations, yesterday and today was that, as the LM accesses data is a vector for itself. Yeah, it's lower cost to inference. And it's very transparent, higher and for many reasons, maybe it's shipping to a skiff or shipping area or shipping to a bias or or developing that Laura adapter and a binary that can then podcast with the pipeline that you may take your relatively structured data medical record, and then compiling that into effectively a binary that ship is part of that lm

Speaker 7  27:10  
a loss I lost myself in this year on cuz it was a container is like package compute in a binary that's like the spring the equivalent of this and where the threshold shipped with cloud compute to constraint was compute memory, CPU and memory. Once cloud got the horizontal and vertical expansion vertical scale. bottleneck shifted to IO, which is your point is the you're going into this data gravity argument. And that was the strategy I've been encouraging is your data is going to gain a center of mass in a particular region. And maybe what you've got on the East Coast is different from west coast, but it's cheaper for me to move containers between these two places than for me to do the grass

John Willis  27:59  
and it might be a good so like, what he was telling me it's like if you look at the chunks, and you keep us keep accounts, which was getting hit another thing I wanted to sort of mentioned like there's a fair amount of good old Blackboard they're doing so and told me a story about to kill a cat. And but then they have an interesting guy just wanted to point out, they're thinking about one of the good examples of defense in depth is no such code that should not get into model versus source code that could Yeah, and then having checks, like basically, like, look, first check is this from a repository that shouldn't have gotten here, and then afterwards do more of an LLM so could you reconstruct that anywhere? Just Oh, certainly.

Colin McNamara  28:42  
I think that actually completes the thought of being a borrower. All our data is loaded inspector dV. Once that data gets compiled in the data in the LM, you know, it's almost, it's a weight, it's a weight, it's a weight and bias inside of there, but it's fairly obscure at that point. And instead, I

John Willis  29:00  
hear source code, like if that's a good canonical example, like, I don't want my source code IP. That should not be should be blocked going into this model. So first off, I wrote check. They don't have a reconstruction. Can I actually reconstruct that protected IP code? After it's gone? It's like almost like a co pilot. A co pilot says, generate me code that does our banking stuff with an algorithm and XYZ. Well, you know, the, if that code came out, then it was it was more likely somehow got into the system through nefarious actions.

Colin McNamara  29:32  
So it's like not only do we have a pipeline that moves data into the application, but then we have almost a second pipeline promoting the vector data into the adapters that are using the current term time sheets. Yeah.

Speaker 5  29:47  
That was why until it's it's not like that sequential sometimes because here. Yeah.

Speaker 3  29:56  
So this is like more of a philosophical question, but I can't get my head wrapped around it. So we have this data. We have this model. This model, we're saying hey, this, we're using this model to analyze this data, but we say don't assimilate it into the model, right. It's like me saying, I can't unsee it. This is this is the thing that I'm wondering about like, are we able to, even though we may not have say, say for example, it has your week, your age? How about that? And you we know that you were born in the same year as this and blah, blah, blah. You've seen each other, but we never implicitly tell the model that Patrick is 29. Right. But it has all this other information that it can always infer even though we haven't put the private data into the model. So that like is that that's got to be the real like, you know, matrix, like issue at the end of the day is like, do we you have it in the vector database tag that is secure data. But what happens is that the data never actually enters the model, but the observation of the data gives it enough information so it can infer without you having that model in there, so I'm wondering, is there some kind of thing that you when you security tag it that we like non inferable data, you know, I'm you know, I'm saying like, like how do you wipe it, how do you do that like

Speaker 5  31:35  
it's not completely wiping? Remember what we said? Yeah, when you say, Here's a few examples, given officer, but then one of the, the things you add to this, like, only base yourself on these examples, right? So so you kind of limit like, in a way that it's not gonna go like the whole reg. It has to be grounded in whatever you provide. I'm not saying that's exactly safe. But yeah, I'm

Speaker 3  32:09  
just trying to think about like, like this whole idea of this private this data streaming in there. They're sort of like, the only thing I can think of as non inference data because this is an inference model. So

Colin McNamara  32:25  
So is this part of in this architecture where we have a set of tools and instruction code in a way where are you describing the tooling on the side that's observing and checking and getting our from a pervert?

Speaker 3  32:43  
I'm not sure if it's a gate before it gets to DL. Or, like what part of the

Speaker 5  32:50  
exercise we can get from saying you should call the business up that we can have things like an MLM. It was like it was kind of style. Yeah, that I've seen that they filtering out whatever it gets returned. Based on that similar filter. In the API itself. You can maybe like do a filtering, right, what's going on with the wire, and then you can go back and kind of say, you know, you're not allowed to use the source documents. So so this is this always is discontinued, perfect. No, but you have different places on your control.

Speaker 3  33:29  
And I'm more like trying to seek understanding because like Collins, very deep into actual using land chain, which conceptually, I understand. Yeah, yeah. But still, I'm like, I'm just trying to figure out is that is that the right like, do you have to keep it?

Speaker 5  33:48  
So another guy saw I don't know, like on the privacy thing is that the day before that feed the data in they anonymize that before it goes into model. And then they they use kind of like placeholders Yeah. And then have the LM answer true placeholders and then they kind of replace that bank with the actual PII data.

Colin McNamara  34:10  
So there's, are there there exist, but it is what I'm hearing you say is there's existing data science tools that can be used on the data's gestion layer for standardization?

Speaker 5  34:19  
Yeah, yeah. There's some kind of tools that we're we're trying to make this more robust, but in the end, if you're in the general domain knowledge, that's gonna be really hard, right? And what you're saying is almost like I can fingerprint a person based on a couple of datas and then I'll be able to infer,

Speaker 3  34:40  
yeah, that's the whole Jasprit hidden. I'm really scared about this because it's, it's actually taking inference to the point where it's advancing farther than they thought for that is

Speaker 4  34:54  
just wanting you guys like a 32nd little soundbite of like, explaining what's going on here. on that sheet in front of you guys. You can do it. I call it just flip the sign you real quick Sure. Microphone basically, just to the camera say alright, this is what we come up with so far, you know, 30 seconds, 40 seconds. And you can put it there, whatever you want to do. Alright,

Speaker 3  35:25  
so historically, we've always had tool chains and pipelines or infrastructure. And what we're trying to do is figure out what the tool chain in the pipeline of the data that interacts with our API looks like in the same way that we would deploy things to the cloud. So we have all of our data over here. We have a layer of storage, it's a vector database. Then we go through our middleware, which is the code we're using something very popular that top line chain, which instructs the LLM, how to handle that data. And then over here, we're talking about this, the, the loop of us developing deploying that data and tooling around the large language model to run them in production and enterprise the I heard it's a rapid SOS government down I know, I know. I'm like sitting there I'm like, Okay, I've met for DCED

Colin McNamara  36:26  
when you're talking about this and looking at the EPA visuals I when I started thinking like okay, like how to hack this together, is this something where we have like, they, I know this we can really get Yes, I think we can we can version, like do we just start chaining together things and get up act with to get out of actions is scripting, a little bit of scripting, a little bit of like bubble gum and baling wire? Yeah, I mean, I don't know like not the smartest

Speaker 5  37:04  
one on our team to add to pipelines overview. So we've been kind of getting this into production right. This the monitoring and till traditional kind of DevOps tools have been looking at latency cost and

Speaker 5  37:33  
so what we find out is, if you're tracking like a

Unknown Speaker  37:37  
call, you would do a log or

Speaker 5  37:41  
be found this to be too simplistic. one API call response. What we found out is it's like fine for you. But it's, we actually needed to hold context of that call before he could actually reason about it. So what we need is, so what documents did you actually use to resolve in this? What version of the model? So we need to have the whole context to kind of understand and then the users environment we need to understand these. What was the first question he typed in and what trade like a threat? Was he kind of going or as a child, why did it mark it that bad? And so that was a great thing for us to record. Because you would typically have only one call and you say like, oh, this is a return? Yeah, it's like more contextual logging that we needed to have. And then we find out that we could do fairly easy feedback like tons and tons, whatever they like it or not, and then also with if we give them a result so they ask us, please generate something and they start editing. So this was a cue for us to see it's not good enough because they're editing this like it's a cube. But it's better than comes up with Amazon because then you can see the edit and but comes up exam you can't see like why did they not like the answer was about appropriate.

Colin McNamara  39:12  
She can see the iterations of their prompt as that reengineering the prompt, almost viewing them as an external LM improving and having that structure and capturing that correct as training data. Specifically,

Speaker 5  39:27  
training data. But it could also be just us like, why is our problem not correct, so that we needed to iterate on problems? Because it was we're returning results. We didn't want it popping up piling up. I think this is absurd. That was great. Yeah. Is that what it's gonna be like? Yeah. That's perfect ball. Traditional

Unknown Speaker  39:52  
career for a handsome Marcus is

Unknown Speaker  39:54  
what really needs a runner.

Speaker 3  39:57  
Sharpie marker, what three floors? What's really true? Yeah. All right. Perfect. Are there lots of customers? Yeah, like we'll see how many times the multicolored packs of marmi generate the three or four times the four packs accidentally. Not just for not just from like, rejoined the province, but actually go and there's not sharp

Speaker 5  40:30  
Sharpie colored markers. Here we could have probably. Alright, cool. Thanks. Thank you so much. Appreciate you guys. No problem. Whatever we need to do anything. I've seen products starting to do. So we think about overcoming promote the data and then it gets like there. But like I found a product called gantry and they're only doing the pipeline of the from the template. So you would not have to kind of change this thing. It is just a pointer in your code that points to an external prompt library. And you're just promoting prompts.

Colin McNamara  41:10  
I've seen I've seen a pattern using this where the prompt templates in the jeans a prompt templates are referenced externally in like a second repo and then pulled in that way.

Speaker 5  41:23  
But they're unable to do that and kind of like a promotion thing outside of the code pipeline thing. So in so then they can just invest this and then with the ad testing production, they're like a mom and fix systems and all this work is that work? And then they kind of feed in I see you thinking and then they're using those seeing this work. This doesn't work than to like on your point to kind of improve the testing and all that because now we have actually bad examples, which is actually harder to come up with some good examples. But I see in our company, we're like, oh, should we buy a product? Or also No. Is it worth the investment? Should we build it ourselves? So they kind of like the whole debate is that yeah,

Speaker 3  42:11  
and that's my whole thing. Like I'm fascinated by Liang Chang, but I'm like, there's gonna be so much here as soon as like, like right now that I'm like sitting there. What is the right way to go?

Colin McNamara  42:23  
Oh, no way. Am I picking a winner in London. With that I look at active community, I look at missing errors and funding. And I look at I can rapidly prototype very complicated code and people much smarter than me you can do in any other language. And I can hand that off to a team to improve them on scale and make or whatever. And that that's the only reason if there's a better way of doing anything.

Speaker 3  42:47  
I think of it sort of like Docker, right? Like, it just became the default and I'm wondering if this is going to be the default because I don't know enough about it. I'm just like, it's so early and I see since we're early, and I see so much cool stuff and there is it's vacuum it's it's sucking up the air because there isn't anything that's good apparently is good.

Colin McNamara  43:05  
In my perspective right now. Like sometimes I've been talking to people they're like so focused on like, what we make money off today. It was I don't care. But is that like, we're in this hockey stick? We're in the world of exponents. I have no idea what the world was. going to change as these exploits happen. My focus is on engaging in collaborating with people along that hockey stick so that I'm writing code and working in hackathons. I'm expanding my mind in perspective so I can understand what that next iteration of it is like. I don't think any of this is where when we focus on it at once. Yeah,

Speaker 3  43:39  
that's that's what I'm trying to figure out. Where do I invest my little pea brain on the stuff that I think is gonna last? And I'm wondering, like, like, the other thing that's interesting is when John started talking about the whole manga admitted data slash vector store, smack. I'm like, I'm sitting there going well, MILVUS and pine cone and promo are the like three most popular ones. And then like, oh, well, now we have to think about every database vendor out there will now retool and you're already got an install base of

Colin McNamara  44:15  
that's going to happen for going after wherever they're gonna take the files a file or put it in put it into a vector Avi, you can do it in Redis right now, you know, and a lot of my strengths is on the storage, like storage side like 20 years ago, and it's it's neat to be able to see like where the logic can go to the left and want to go to the right. I feel like I'm digressing. But so

Speaker 5  44:41  
what are we trying to do is you you kind of look at

Unknown Speaker  44:47  
existing brands,

Speaker 5  44:50  
a little bit about their legacy. So for us, for example, the choice here was, while we have OpenSearch open source has embeddings. Right? It's been there a long time actually is being backed by AWS. This is never going to go away because it was going to before it's going to hit there. So we'll better be the fastest the innovators in that embedding space. No, but you know, kind of like it's hard. It's a lot more than it hasn't been to be more so. Here. We're guessing. While you know a lot about this is the learning and not the actual framework. So you know, the replacement but watching your writing is not that complex, but the problems and kind of like dealing with the models. So I'm not too worried about the cost investment per se yet. And like changes for free. So I'm kind of like yeah, the tricky part comes because with like all these vendors, like if you bought a you're stuck for a year contract, and then suddenly that'd be you become like, do you have like a sauce per request kind of thing? We got lucky here. Just saying it'll be asked by the disco. Yeah, we're, it's been a decision it made our life a lot easier. But we know they're going to be open regarding their investment in privacy, security and all this stuff. And they're gonna go against you know, open AI. So so that was our that but now the biggest question is all these tooling around. We're experimenting, or like, how, what licenses? Do we need to pick and if it's for a year or not? And there is like I would look a little bit more to the innovation side, because I'm probably gonna learn more effective innovation tool right now because the other circumstance, they're not thinking that it was a way of working better. That's kind of the reasoning like picking tool.

Speaker 3  46:47  
So it really so really, we got

Colin McNamara  46:54  
sorry, I can see your mind crunch crunch. No serious what's, what's going on? Well, I'm just it's your honor, pressure. I'm sorry to put you on the spot.

Speaker 1  47:14  
What I'm wondering is like, how much data would we how much information would we need to say debug something? Like once we once we understand that it's constantly or mostly like giving out like bad data? Right, because we can we know like the further down the line one goes further down towards production is it's more costly to usually to fix that problem, right? But how much data do you need at that point? Or how much? How much information does somebody in your company the at that point to understand where the problem lies? Because this is a huge pipeline, like the problem could all be all the way at the source, or it could be somewhere in between during the transformation or something like that. And there's so many layers because it would just remind me of like a CI CD process, the more layers you have to finish the entire process more. It's like exponentially more expensive against to fix the problem and also it becomes exponentially more difficult for the operator to triage, if you will, like down to like, this is the problem and these are the people I'm going to need to get the state fix

Colin McNamara  48:36  
well, and so let's let's think about it in the context of like multiple ways you can structure the code of this logic, you know, for now, we're talking about a simple sequential chain. But okay, let's let's say that we moved to tools and agents, right where your tools maybe your retrievers, and your agent centered their prompts to specialize. Those agents may be wearing another agent to get a next steps in executing and executing more steps. And they have a complex interactions with very hidden in the code, right. And so, in this simple case, it's complicated but now as agents themselves maybe moving left and right, like how do you gain visibility in that how do you optimize it? Do you do we establish a rail to the side which is like a, you know, a debugger debug AI? You know, a whatever that is both pulling the information out but also providing insights back.

Speaker 3  49:34  
When you say agents, are you talking like agents in the traditional sense, are you talking about like autonomous AI agents that are smart that says, we're going to make decisions right here,

Colin McNamara  49:45  
I'm talking about agents as defined in. In lane changes my experience, but ours can be written in Java or Python or whatever or whatever language but an agent that is that has a prompt template inside that gives us a personality that has access to tools, which may be like Fallout, the tool or retriever tool, whatever and that understands its relationships to other sections of your code or piece of pieces of application, which, which may be accessed within within the internal API's or maybe through a bus. Right? So you have a series of agents that we programmed to take actions to do something with, but they have a little bit of self direction.

Unknown Speaker  50:30  
Okay, that's that's what I was getting at. Yeah, not

Unknown Speaker  50:33  
fully standalone. But maybe

Speaker 3  50:34  
maybe I'm like, that's what I'm, I'm, yeah, yeah.

Colin McNamara  50:38  
I mean, the base. I have a use case that was like a test case where, which I think is relevant to the enterprise where my two agents that have that use different two agents right here. And inside they have, each of them has a different LLM chain to find and then that each of the LM chains has a separate vectors vector store, which is pulling in different types of information. One was a publicly addressable speech. by the President. And another one is technical documentation. And then there's a concatenation of basically a run job run in front of it, which is identifying taking one string which needs data from all each of these each of these agents and agents are going and pulling information. And mashing the matching, putting pushing through the button place and then returning effectively concatenate the string that's that is a concatenation of the terms for both agents. Right? And in that case, like how the hell do you look at that? Right, how does an operator

Speaker 5  51:43  
Yeah, but I think it's complex. But is it any different than let's say in today's enterprise, you're using 20 different external services, which you don't have any visibility which you're using as well, and they can fail and you don't understand why and so on. So if there's a beautiful so the thing is, you just apply the same things like integration testing, unit testing, like the theme itself, integration, would it provide a mock? Would it provide the documentation or do they provide you like the visibility and so that's kind of the that's going to be the same patterns? And I think, you know, you're kind of reusing this as one block and saying, you know, we're getting it out and it's going to be efficient, and so that that's the way I think we will have to cope because if you're going to go on all the errors, we're gonna go into too much. You're not testing the hardware layer and kind of the CPU and all that stuff. So for after a while, these will be like, blocks that you just say you're moving, and I was in this italic, like they tell you a little bit of versioning but not entirely like started here, but you didn't. You don't get to know. So even if they're releasing continuously, their version is still kind of like sequential. And that's kind of how why I think we're going to deal with

Speaker 3  53:07  
so we're gonna have this random stuff that sort of sent to us but there's gonna be agents that do sea ice manager, sea ice, CD, the rentals or ability maids will manage all your data streams and all that. So they like what I'm thinking is that you're going to have an explosion of these small language models for lack of better term that have specific tasks on the edge that do stuff

Colin McNamara  53:34  
small agents in your pipeline. The air is agents managing angels.

Speaker 3  53:38  
Yeah, that's, yeah, that's sort of what I, that's my theory is that it's just gonna be we're gonna take this sort of, you know, distributed approach to it with little bits. of smart things and then in the center is almost like the Uber lol I think there's gonna actually I don't think algorithms are going to be as interesting in two years as multimodal models because that really I think, is where it's gonna go.

Speaker 5  54:04  
So, what you would say as Remo is I would say it's probably going to be the coordinator. So yeah, it's just another cific task of coordinating a synthesizer. Yeah. That's, that's how it was. You see the approaches of multiple agents finding consensus. Yeah, there's one who's kind

Speaker 3  54:24  
of Splunk is going to come out with some kind of like, they have all this stuff. I'm just thinking about the people that are going to be really good all the way around are going to be people that do all of these tasks and have data on it to train these great minds open source community. Yeah, I hope so too, but I don't think like in certain cases. like security, like, sneak and take all of their data and create, like, some kind of, you know, security scanning for certain levels, and there's other security firms that do you know, pen testing, and they're gonna have their fingerprints like,

Speaker 5  55:00  
you know, open AI or X amount. of models. Talk to the copilot there underneath are like 25 I think that's very

Speaker 3  55:14  
I just, I just feel like we're so at like, I look at it, I'm like, I still sit there and watch the stupid problem. I put in my prompt and I watch it type out. Like, like, I know, I should go do something. But I'm like, every time it comes out with something good, I'm fascinated every time. But I think that then what's going to happen is there's going to be these very, very, you know, it's just like going to a college. They're going to be professors that are so deep in something. There's going to be models that are suited based on that and then what we're doing dealing with today is just going to be coordinating models.

Colin McNamara  55:51  
You know what you're saying makes me think of oh, gosh, intelligent cloud and Nick Nicomedia. Like, it was architect for their for their open cloud. Find this observer decider after model that I've seen that model often model hyperscale clouds, right. So you have the code that's observing the operation of the distributed platform, that a portion of the code is deciding whether that ops what should be done in the observation and passing that to an actor that was changed and you can interject humans in tickets in actions in any any one of those three, and create new we're having a conversation with but there's the human as agent model, where you have to actually define an agent in the in that in those agents that talk to each other that is actually a human interface. It's like popping a ticket coming up to me after the thing in I wonder how that this framework decider, actor, agent model, we sit on that rail,

Speaker 5  56:55  
there's no thank you. So. So we've got some markers. Yeah, look at that. Nene talks about kind of this pre generic region, content that we're providing you with I think as an area, which is so all the analytics are doing so it's almost like the we mentioned eat are you trying to context but you keep tracking what the end user does. So is like the trail and clicks and then based on these things, you actually like that, that's another source of information. But we're it's more like you you started tracking everything. And then you get personalized things. So it's not your your generic doxa loses bucks. So like when you're going into production, one of the chapters is deep learning. It's not about the API or you need to capture the full context of flow control. Yeah. You need to care what problem with you what data set was provided? What was the user doing that? Before you can actually do a lot better? Because a lot of things are just showing this call and that's it. That's awesome

Speaker 5  58:42  
so this, this stream will be used to pile them up in our in our data lake like they're all the things people do. But I think that will play an instrumental role of memory for native for user. So it's like, all the things he observed that they've done.

Unknown Speaker  59:06  
So the answer will be based on

Speaker 5  59:10  
Yes, yeah. And most of the mutation has been like you always know, watching my behavioral data that we're using actually to kind of make this more useful to what has been doing. So it fits a little bit. There, but it's kind of like behavior.

Speaker 5  59:37  
So all these kinds of assistance, you know, Microsoft, they're basically listening in on what you do all the time, right? And then they are recommending it based on your context and your content. So that's why we always make a distinction. It's like content and context capturing. We we do that as much as we can. But this is a little bit like the memory or the memory

Colin McNamara  1:00:08  
so out of all this, you know, as we go back, we come together. What is an output? We would like to deliver to the overall team is a recommendation architecture is it a working example? Is

Speaker 3  1:00:29  
it as I feel like it's architecture or at least architectural principles? I think that's the whole problem is there, that's why I'm like I think there's this that's why I'm I'm like the code part, I think is always separate, which I don't even know middlewares. Right, other than it's just everybody knows what that means. But and there's always going to be data and there's always going to be a model. And I hate to say albums, because it could be you know, it's gonna be any j it could be a vision model. It could be

Colin McNamara  1:01:10  
anything so I mean, I use transformers models, my business process right now for prototyping consumer gets easily right for the versions of that incredibly

Speaker 3  1:01:21  
important and I think that it almost looks like there's a data layer, there's a middleware layer, there's a model layer, and then they bounce back and forth between sort of hybrid classes. There's stuff that you are maybe more private I don't think I don't think it matters where it's executed. I matter. I think it matters, privacy and the level of control you have around it. So that's sort of how I think about it is just that matrix of, you know, your data, your middleware, and your model, and then whether it's private or public, and that it couldn't really say like activity. Yeah, it's kind of strange. It's a little loaded because we think private sometimes you think like, your on prem data center versus now I think that people figure private could be, you know, like bedrock is probably a privately maintained model and Cloud

Speaker 5  1:02:30  
products inside the BBC. Yeah. Like the tenancy quality, like all our customers have their own set up. So they would have to have their own model environments and kind of like, that's how we're able to do

Unknown Speaker  1:02:44  
floaters. floaters,

Speaker 3  1:02:47  
which is this company that fascinates me and then I guess they're gonna announce their CEO next month, but it's confidential computing company called opaque systems. And they're asked rotation is that they can run your data within the model confidentially inside of their whatever can take it in, which, to me just seems like it's back to my Can you unsee that kind of, like, how does it translate back and forth? That's part of the private, private public thing is that you will your data, go into a model and be processed in a way where it's still private, even though it's like, like sharing the GI

Speaker 5  1:03:30  
Bill, that could just be retrievable, right? Because that's basically what I'm adding to the moment. It's just,

John Willis  1:03:39  
we went I went like to war on a conversation with Joseph yesterday. About like, like the layers of like, like, in other words, can I trust pine? Well, first off, can I trust Who can I trust? Can I choose Open AI? Right, like how much do I trust him versus as your open AI? Right, and I literally took me like 40 minutes to get him to say because he kept saying depends, depends. That's when he showed all the slides. But I was trying to get to like okay, if all things being equal and they never will be the if you were highly sensitive Corporation, and you basically had two choices, it was open AI or Sure Okay, which one would you go to? And he finally is like, yeah, of course I go. And then the question is, Okay, why don't you trust overnight? Well, I didn't have the hygiene net that Microsoft has. And then the question is, Who do you use for a vector data store? Would you trust MongoDB over pine cone pine cones really early in this game? They don't know what they don't know. You're like, so I think there's so that could like they could say we can protect all of the world but if they don't have shadow leeches, there is their seaso. Like, who knows, like the things they don't know. And even though they're not so even if they had like, the greatest numbers

Speaker 3  1:04:54  
that you're playing the odds Yeah, they're better at it, then. Fine tune but

Speaker 5  1:04:59  
it's one of the things we sound like all the like, instead of having people go directly to the factory databases, and so on, it's just like, everything's got to have an API to get to it. Because basically, that's an access control layer that's been matured. If we're all reinventing our own access layer, all those things. We're not gonna like sometimes you have to. It's like we're trying to make this real exception

John Willis  1:05:25  
has become a new cause in his diet architecture diagram, that thing where he didn't really have a wall and he said right now it seems to be laying chain between all the API's. And wonder if there's a new type of API gateway. Or maybe it's the same one or

Speaker 8  1:05:40  
we had a little bit of website chatting about this, like, if you think about quality, right, you know, the same quality. You shouldn't even let that data into MALDI. Right. Like in models like the data store is not getting it like the source material. Shouldn't even make it into there. So like we were talking about publicly they basically private, non sensitive and sensitive material. And then basically having an our back as far back before the ingestion of the material are basically saying okay, yeah, based upon either the source or the target. And there's a third dimension which is the person so you have source target and or a person source target. Can this come in? Can this be baked into the model, and they can remove that all the way back to the furthest part of before we get into the database can this context avid then you've now that you've basically definitive as far back as you go? It's

Speaker 5  1:06:26  
one of the reasons why for example, when for the embeddings, hopefully, I think you will be I think, links directly into the IRM as well, so we can like see who has access who gets what data, and that's supported in the whole platform for us, so we didn't have to cobble our own custom security model in there. And the same for to retrieval, right. It's like it's, it's integrated. And if they're all in like, you have to integrate this across all these different things. Have fun, guys.

John Willis  1:07:00  
I got a question. So those these guys sort of wrapped up already and like I want to be ready to go as long as they think they need to go. Do you have a sense of kind of summarize, okay, so you think four o'clock might make sense as a as a as a gathering, if everybody else is sort of in line with that so we can start early on our debrief and, and see where we're at. Okay. And what I definitely want to push any team to premature

Unknown Speaker  1:07:25  
we can do that. I don't want to be pushy, but I'm gonna push it.

Colin McNamara  1:07:31  
So in the conversations that we've been talking about, in we've talked about overall overall architectures where data logic model and you're an application performance company come in come together, right and then as we talked about this entire thing, how to do version testing artifacts, deployment and analysis, right. We talked about the the inspection in we talked about the need to segment each of these areas with API's establishing access control, logging serviceability, and to drive to pull that and bring it into a service to like a serviceability layer in our observation, right. You know, we I've been capturing down kind of the deliverables, what would possibly come out of that discussion as there's architectural principles, there's a model or almost a reference architecture of how we can talk about this. There's a definition of the access control layers within that architecture and I believe I think we're starting to talk about either roles, as we're as we're now using fulfill our kind of model based model based application structure that are testing rails of agents on the side that are augmenting the operators and machinery to the within the kind of the observer decider, actor framework that that we've seen in hyperscale. There are other deliverables. I think that I think coming up here I feel I'm missing looking for feedback for others. Like we're going to present this back to the team. You know, what, what valuable insights can we can we capture consolidate from this?

Speaker 5  1:09:20  
So you've got to find the context and behavior data, put it there. So it's a different set of memory of what

Colin McNamara  1:09:30  
users have been doing. Okay, so there's a there's a new requirement that that is that that's emerging from this and applications that that the the context and behavior capture in what's the right word for it. Let's start All right, do context behavior.

Speaker 5  1:09:45  
Let's say it's a new environment, but it's a minor set of data. That's not often considered.

Colin McNamara  1:09:51  
So there's really but there's new data there's there's new data lake has to be created. There's a new infrastructure that has to run to be able to capture it. There's

Speaker 5  1:09:59  
I would say it's already there. Usually instead of analytics or what customers have been doing, but that's useful for personalization. Because you know what the user has behavior. You have your knowledge, chesty data, and you're kind of combined. So I see that as a memory, things.

Colin McNamara  1:10:19  
Memory I like that word. Memory of the application.

Speaker 5  1:10:28  
Something we didn't touch was the so this is using prompts to massage data. But I still feel this is gonna be like an expansion for you feel.

Speaker 5  1:10:47  
Like it's almost like aI functions and what I view it that it's, it's using LLM techniques to actually performed things we would not have had to code before. So most people see as you have footpod or something, and it will generate a snippet of code it all pasted in my code. But imagine the prompt will be mentioned in the past you would have to write sentiment analysis write the model law. And now you just say please, LLM given this input, what's your sentiment? Like? It's just, if you're not writing code, you're just giving that prompt and it is a function. Sentiment Analysis, boom,

Speaker 8  1:11:34  
almost like the constitutional AI stuff as well. Similar, like it's a one it's one use case that

Speaker 5  1:11:41  
I saw somebody starting this talk as just, you know, enter a prompt, give it the function A what are the inputs, what are the outputs? And it's like a lot of that but even without decoding, it's deployed as the function. Yeah,

Speaker 3  1:11:54  
I think that AI function sidecar is gonna hit every layer of the stack. But it's gonna be from the data organization and its data lake, to the filtering to the Managing Your, your versioning all of that I just feel like everything is gonna eventually trickle down with that AI function. Because

Speaker 5  1:12:15  
the other example of, you know, I don't know if you've ever played around with FFmpeg. Good, apparently. Yeah. So there's the example now is that you give it a text prompt to say like, I want this file to be transformed into kind of that, like type of just write the prompt. It's hooked up with the API's. And here's the function, right? It just runs that like, I know it's influencing our I think we think about recruiting, if that were a card could specify what it needs to do. Maybe not ask the right function.

Colin McNamara  1:12:54  
It is a layer of abstraction right? I think two were the biggest advances in innovation happen when you can put a level of abstraction between the user and the code and think of the word processor, right? Still, yeah, if I if I think of Google Docs, I think of the spreadsheet as one but then the great example here. So now we have the ability to define functions by R. So I am requesting that I request LM to create code to create a function that does is Is there another management framework that comes around this? Is there a new way of looking at how this data was created?

Speaker 5  1:13:39  
I haven't seen too much of that with internals. I've seen a lot of examples. Like one isn't the course and also I'm sure it's going to be more interesting because one of the things you get here is all that kind of JSON formatted. Kind of like that, that will address as well. checking whether the author's for this company so this was this was habits on chain.

Colin McNamara  1:14:10  
And then what happens was up until recently, software development has been captured by people with the tools training and education, to understand algorithms and to understand the life cycle. And naive instances just like me today, sitting in the back of the room trying to figure out how to how to get to take LM output unstructured in Markdown in a reasonable way and I'm interacting with complexity. I'm writing I'm going back and forth between an agent from an LM and my code base going back and forth. Now that in a business context, we go from 10% of their of the staffing developers outside Silicon Valley, to 60% of the staff being developers. How does that change fit in this framework? Now that we have a lower quality input in much more volume,

Speaker 1  1:15:05  
more senior developers like it because you just trust like whatever the LM is going

Unknown Speaker  1:15:13  
to generate. I am right now thinks about idea. Yeah.

Speaker 1  1:15:18  
Because you need more subjects

Speaker 5  1:15:25  
around this is by having all these arguments, okay. You have the LM are working with itself. Is this good? Is this like what does what it does? And then if not, try again. So you're like,

Unknown Speaker  1:15:39  
where does that fit in here? Where's that critical? It's somewhere. Is it there? Or is it in the end isn't? Isn't there?

Speaker 3  1:15:53  
Just implicit if the networks are using again, I mean, is there already that's how they work great. They're generative AI or adversarial networks are going back to certain rules within their their models themselves.

Colin McNamara  1:16:07  
But that's down here.

Speaker 3  1:16:09  
But that's what if we all have side cars like like, Patrick's AI function sidecar there? Do you have one that is very much at hearing other critics creating sidecar for your code? every litre Python and PHP code today, I should never be allowed to check in any code anywhere. It's like so rotten together in this

Speaker 5  1:16:45  
room, right? So we have the same cycle to say, this is our goal, the documents of writing the code. This is the context that we're doing. And we're feeding this to that same URL, and we're not just doing business. We're like writing code, and we give it all the kind of feedback and the knowledge

Speaker 3  1:17:01  
and this is actually just me. I don't want to be overdone this is I just feel like there's gonna be trickle down. Trickle down AI into all of these things. So right now, we're, we're solving for today, but I think tomorrow, what's going to happen is these agents are going to be everywhere and they're going to be very, very, they're going to be specialists just like we have.

Colin McNamara  1:17:24  
I would say that's good doctor, I would say that's built into my assumptions that right I just right now, I am an AI s accelerated worker. Yes, that's what I am right now to date. I sometimes do things by hand. But most of the time, I'm either using myself in the company of copy and paste methods, or I'm writing some code that executes an API over time as his code becomes checked in. As his code becomes running on a cron job as a next step. And over time, this is become

Speaker 3  1:18:22  
So I feel like we're moving from task oriented AI to goal oriented AI, which is that sort of autonomy that within within some kind of box

Speaker 3  1:18:36  
if you've ever played with auto GPT, or maybe AGI and watch it just continue to spin and spin and spin and spin up and run because

Colin McNamara  1:19:27  
Yeah. Well, I think this brings up something that as we end up with exposing these application models to the larger base of users who are not calling you naive, but I'm calling myself naive when it comes to interacting with these application structures. I've learned these lessons two, yeah, my API limits is a little lower. So it's gonna cost me 20 bucks at a time. But we have also the cost controls the data exfiltration controls, we have a whole a whole other level of user base that we have to apply, like, our software development lifecycle methodologies to it, but not in a way that will really accept training to the point of pipelines, how does that fit into a pipeline? Or it was just talking about this

Speaker 8  1:20:20  
when he asked the question, why should it? Sorry, I'd be meta here, but like, we just assume automatically everything has to fit to a pipeline or some type of like that. So why would they just mean by just thinking a lot of why wouldn't have the fit of my plan? What's the value of pipe lengths? Anything the pipeline is unidirectional. Left, right. It's just more network based and is sort of like a value creation, delivery valuable. And we also think a bit of a talked about from project to product.

Colin McNamara  1:20:52  
So I might be using the wrong terminology. Forgive me if I am. But my end users are improving prompts that are that are and Amadeus living in this business logic layer. Right? And the structure there prompts might dramatically increase the cost of execution, right? That my first thought of where we were, we would have control and control that costs in control, whatever security control all things would be, in the existing products processes of promoting this code in production and pipelines would be the first way that I would think of solving that I might be completely wrong. So that's my thought, where why a pipeline would fit in that

Speaker 8  1:21:33  
pipeline. Lee come back at you, I I'm having a cognitive disconnect on that one. Okay, I think I understand you, but when I think of the pipeline, something I get like a pipeline is just sort of the vibe going from bits and bytes into something that can be consumed, like in the software manufacturing. On validating that a set of declarative assumptions that this is ready to be run that to your perspective, if a prompt generates 50x More turn on the execution. I'm not sure that'd be

Colin McNamara  1:22:07  
when I'm saying that and not just a user entering entering a prompt in a web interface. But as like a business user, who now is updating a product template in the context. This is the new new pivot table. This is the new Microsoft Word for Yeah, right. And that I see in my own experience, and in what and how my own journey along this has been like, okay, good. I'm structuring my application and now I'm improving the application improving the logic that I defined for my agents. And now that further and I didn't know if you came into this further part of the discussion, but as the, the boundary, the barrier of entry to writing code is now drastically lower now that we have our AI code coaches would not, instead of having 10 to 20% of our company writing code, we have 80% of our company, right? And we have a larger amount of people updating the definitions of our agents in human readable language. So that's where I say okay with a bunch of people writing code that's going to end up released into production, either internally or nine. I personally assume you're on Don't play me right. That the natural place for that is where we gave our software developers with that behavior.

Speaker 5  1:23:23  
So I believe it's still 500. I don't see. I see the pipeline always has the feedback. In that case, you learn that it cost more we found out one of the things if we didn't come straight into problems from the end users in the there are GPU costs window. So the fix was No, we were monitoring cost was strange. And then we change the code and then put a restriction fee, right. So it's kind of like that. That's still like a feedback cycle. You go in, and then on the agent one being a pipeline, I think it's just it is maybe a pipeline but it's not like a predictable pipeline per se.

Speaker 8  1:24:14  
Use the feedback cycle. I think the key is like not getting in there and like the reason I never see

Speaker 5  1:24:18  
a pipeline, the feedback says it's like always kind of

Speaker 3  1:24:22  
Yeah. I was just gonna say is it today? I think it's centralized and very hard fuel as in our traditional pipelines, but in the longer term, I think it's more distributed. And so like right now, it sort of implies there's a central business logic, versus I think a lot of that stuff will be broken up and it will be more it will be a flow processor,

Speaker 5  1:24:50  
but all the different pipelines need to be triggering on pipelines. Yeah, yeah, it's not a big centralized, right. And you already have the versioning. Like one is bumping the other needs to go and have a conversation or you have a contract.

Colin McNamara  1:25:05  
I think that that ties in with Steve's focus on the software supply chain. So we're looking at the libraries that we consume in our applications right now. Well known examples where that's caused all of us to work way to me like that we have almost an agent supply chain. We have a logic supply chain, a promise supply chain, where you know, like prompts injection being some of the we pulled into the experience recently, of a series of pipelines was resulted in prompts which are then referenced either through an API abstraction layer, Agent agent, or through coteries and other in other ways. I would suspect that to Bill's question that utilizing pipelines to secure our AI, or AI, hrs agents supply chain there's only two I can think of, maybe there's many more anywhere

Speaker 9  1:25:57  
because the supply chain in terms of the model as well. So like suppose, you know, it's discovered that there's some licensing issue with a particular open source model is trained on data that you're not supposed to be using a commercial context, right? How would you go about tracing everywhere that went in and you fine tune it in this context, you've, you've layered it into a more complicated workflow, you know, for your right now we're not tracing.

Speaker 2  1:26:21  
So do you have like a what what is the that the new S bomb to describe?

Speaker 9  1:26:26  
Like the some of the standards, groups like the cyclone DX group is working on? I think they've published a draft, AI bomb, right? Where you can trace trace this sort of stuff. I mean, you could write down like, you know, and then there's the question of how do you generate these, how do you have confidence in them, but you could at least write down here's what we know about the provenance of this model. Okay.

Colin McNamara  1:26:50  
Is that something that fits into this framework? I mean, I guess that's bombs are the existing

Speaker 5  1:27:02  
it's like, you know, they're all have separate pipelines and that integration pipelines, and all of the things we've talked about artifacts. You can like, build that on things and another one, not saying it's yet activity, but like, for example, on hugging face. One of the common themes is that all models have a model card, which is basically a labeling on flack levied on copyrights. So you can like start printing that but again, it's up to the humans to kind of like you put that right in there.

Speaker 9  1:27:39  
Yeah. Yeah, we'll move towards more standardized places.

Speaker 5  1:27:45  
But, but a dependency graph is it's just one more component. It's especially

Colin McNamara  1:27:53  
driving back to the drive up the question is this is, is are there enhancements to the existing model that we need to take into consideration or is this where we draw draw the existing model and clearly draw where the parallels and like integration points from that are?

Speaker 9  1:28:08  
Yeah, I mean, I think a lot of it is the second like I feel like in general, with machine learning and data ops and everything like that. We sort of know what the best practices should be based on what we did for software. It's just, you know, we're not necessarily as advanced to doing them when it comes to that domain, right, you know, having pipelines having auditability having things like as far as and provenance reported and, you know, searchable, like, we just tufting we've done it all before. We just have to do it again, in this new context.

Speaker 5  1:28:39  
Like, another interesting thing was, so I talked about the company called so to call Obama and they take like the Dockerfile approach to a model. And it's What can be interesting in that perspective is that you see the base model, and then the layers that put on top and then the prompts they put on top and this is a releasable entity. So it's kind of clarity, kind of describing what

Unknown Speaker  1:29:12  
yeah, you have to

Speaker 5  1:29:13  
check something which allows us to check your validated sign

Speaker 9  1:29:22  
that you have that information that information color coded it's declarative.

Speaker 5  1:29:31  
Trying to get the things we learned trying to apply it to the newspaper I

Speaker 9  1:29:38  
heard a lot about laying chain here, you know, like I was looking at, it's as what Python and JavaScript are the two. There are people deploying it, like I said, like, I don't know, we work with people that are deploying models like production, or maybe no one's quite there, but like, are they

Colin McNamara  1:29:57  
building it in Python? From what I've heard and is that people are using Pro prototyping synthetic Chroma. So the some of the materials for Chroma are saying that there's early enterprise implementations where they're figuring out how to make it persistent, how to make it work, horizontally scale. And the same thing, the same thing with with link chain, there's a large number or there's a measurable number of simple web interfaces that are in front of like open AI and stuff like that, that people are building to make that $30,000 A month we're making stupid image, that type of stuff, right? People are crying out for the money, but for like large, horribly boring horizontally scalable, performant applications. They're not. It's not it's not being used to that level yet. We're

Unknown Speaker  1:30:43  
we're just getting into

Speaker 9  1:30:47  
getting into AWS. Okay, so

Speaker 5  1:30:52  
the annoying thing is sometimes in a Python, machine learning things, they they download models every time they think it's a nice thing to do. So you kind of have to kind of check what that's doing. And then you have to bake that into your layers. So it's not doing this. This

Unknown Speaker  1:31:13  
is harder when you're flying.

Colin McNamara  1:31:15  
My bet personally is that because of the abstraction complex code into libraries, like be consumed and if I thought displayed in Pythonic manner, that the two things may happen. One that refactoring code from one to another using an LLM will mature in the hockey stick at some point that being able to rapidly prototype as a simple business user and transform that in whether that is using this as a specification for development teams to implement in a more scalable way or the development LM to that's my gut feel of where the direction to go and based on how less often things are failing when I tried to do that myself. There's a

Speaker 5  1:31:59  
tool called Flow wise, which is kind of like doing a local land chain thing. Like you just call him books and I put my chambers down, you can have that whole chain working. And then the other interesting approach again, is there one side thing lamp chain actually split their library in experimental and non experimental security that some of those calls were generated kind of recently. But you can also put like restrictions on restrict Python I've seen people use wasn't to kind of control with function could be executed. And so they kind of like containing whatever that could could be doing. Control and such. But yeah, I would say yeah, there's the traditional input output checking validation happened in January concatenating. Strings, you're using a framework, because they can call the API and data stores and so that's kind of very similar. But yeah, I think

Unknown Speaker  1:33:15  
I think that's proceeding issue.

Unknown Speaker  1:33:17  
Maturity might be

Speaker 3  1:33:21  
so we have about seven minutes till we get back together. You want to be the speaker for the group. You did a good job of summarizing that.

Unknown Speaker  1:33:30  
Oh, gosh, I want to be speaker for the year when they hear the smarter one

Unknown Speaker  1:33:33  
or if Patrick

Colin McNamara  1:33:35  
is good, I'll do my best to take her in. Do my best so just ask for support or up there? How are we going to help us we're good are we taking thing take her are sticking us the walls there?

Speaker 2  1:33:49  
I think that you're ever there is the good summary Thank you think. I think it's gonna start I'd like to have the picture. Take the picture. I still think bathrooms tomorrow. I'll tell you a little smarter by presenting value I'll jump in show that throw that there's a lot of cool.

Unknown Speaker  1:34:16  
You get a dark How about

Speaker 1  1:34:22  
lots of information. I haven't I haven't really touched on a lot in general. But like it's just it's very interesting what it can do. But I see that it's also possible to magnify a lot of the current problems enterprise have, right because now especially with this last one of this API function, like generating more code generating more code that can actually that can go out of control. Right and quickly, right. And that's why like I'm thinking like, if anybody can generate

Colin McNamara  1:34:59  
the meta of that the code generating code that generates code for clock

Speaker 4  1:35:03  
will Yeah, they want you back in the demon

Unknown Speaker  1:35:06  
generating data. It's getting out.

Speaker 1  1:35:09  
Yeah, but so how do you make sure there's proper like, controls or like, humans need to kill humans. In the right spot. You know, I don't

Colin McNamara  1:35:26  
that's where not a reason statement. So like, qualify this. But the the human has an agent model where one of the agents find an agent talking to agents pops out to a ticketing system pops out to change control and release approval process stuff like the promotion sled, we have structures, we have structures operationally to handle that in like a post back in,

Speaker 1  1:35:56  
but you probably as things get more complex you're probably going to need more and more people that are how do we call it like? Maybe not like seeing, like, you know,

Colin McNamara  1:36:13  
how your code review has your years you have to be like top notch and

Speaker 1  1:36:18  
onpoint Yes, exactly. Right. Because otherwise, it's just as easy to write code that's insecure and does work. Right. And it's easier to make that

Colin McNamara  1:36:32  
work or it is I mean, look at the look at on the crypto side, the DNS out there that were writing really bad crypto, and writing bad smart contracts and leaving them out.

Speaker 2  1:36:47  
They were writing code that was that worked and there's a whole industry that came out taking advantage

Colin McNamara  1:36:58  
of smart people. It's normally the assets or targets to leverage or to liquidate. Yeah, so I want to talk about this in our discussion. But you know, one things that fascinates me about the flight away from the cities what's happened over the past couple of years. Chamber changes society is that 35% increase in business formation from couples that have gotten away from driving an hour each way each day to work in the city to sleep in the burbs to, to pay for health, healthcare and daycare, they can't afford to have kids. And so there's a giant movement away from the coasts into the Senate seat all the time in Texas, right? Yeah. People that are not going back into the office that have started their own businesses that are former developers that are forming big city people, and that they feel are people like me, that are writing bad code. Or that are that are creating functional code that executes needs of their business, but the attack surface is now very, very broad because so many respondents

Speaker 1  1:38:16  
and domain experts do XY and Z. I don't know TerraForm Well, generator has like a security vulnerability, especially.

Unknown Speaker  1:38:36  
Yeah, absolutely. Absolutely.

Speaker 1  1:38:45  
If you don't, you don't want me to write and commit code in JavaScript without without something to do COVID.

John Willis  1:39:11  
Makes sense

Unknown Speaker  1:39:12  
for Dr.

John Willis  1:39:14  
Christine and Vicki to the restaurant.

Speaker 2  1:39:17  
Yeah. Ya

Speaker 7  1:39:30  
we're gonna probably so does everyone have their contact information?

Unknown Speaker  1:39:50  
Okay.

Unknown Speaker  1:40:14  
Literally

Unknown Speaker  1:42:01  
right questions during the middle of the webinar. Great isn't properly that's a great idea.

Unknown Speaker  1:42:23  
Thanks day like the second film was

Speaker 3  1:43:23  
like, eating project with Jean size and I'm like 30 books which I never picked up. Rarely

Unknown Speaker  1:43:35  
unless I started here, when I do stamps and these this is not a special guy

John Willis  1:43:57  
I'm sorry. my boss's boss, like the wife dependency project.

Speaker 7  1:44:16  
And, by the way, this guy was still at the hackathon project. Launched on lambda two. It is updated all the records compliance stuff is three months longer fighting just according to gender get agreements.

Speaker 2  1:44:43  
Like so much better for your eyes. I'm so proud of you. I've been using your nasal spray Yeah.

John Willis  1:45:04  
All right. Start with one

Speaker 10  1:45:13  
figure for one all right. All we want to do is have conversation tell us what happened today. If you have something or hold us

Unknown Speaker  1:45:34  
over here

Unknown Speaker  1:45:40  
to take a picture

Speaker 2  1:45:43  
and then just plug it in and just brought it with us

Unknown Speaker  1:45:52  
so that we can be pretty careful with you guys. Talking about what your

Unknown Speaker  1:46:03  
big one that's great.

Speaker 10  1:46:41  
Okay, take us back to what number one actually was

Unknown Speaker  1:46:47  
governance

Unknown Speaker  1:46:56  
laid on the ground, laid on the ground

Unknown Speaker  1:47:13  
and see more thank you

Unknown Speaker  1:47:27  
actually, the answer was one intern

Speaker 4  1:47:35  
just started on time rigidly and told her

Speaker 10  1:47:40  
not to minify governance that has improved Oh yes, I'm gonna do

Unknown Speaker  1:47:50  
the same problem as since I'm switching back.

Speaker 9  1:47:54  
While Yeah, it looks like it switched to the change the resolution of my screen. Looks like seeing the monitor Schumer

